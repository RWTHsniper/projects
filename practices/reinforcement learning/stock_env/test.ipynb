{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    For a given observation, compute Q values for each action\n",
    "        Attributes\n",
    "    ----------\n",
    "    lr: float\n",
    "        learning rate\n",
    "    input_dims : list\n",
    "        [8]\n",
    "    fc1_dims : int\n",
    "        fully-connected layer 1\n",
    "    fc2_dims : int\n",
    "        fully-connected layer 2\n",
    "    n_actions : int\n",
    "        the number of actions\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions, weight_decay=1e-5,p=0.5):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "#        self.dropout = nn.Dropout(p) \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#        self.loss = nn.MSELoss()\n",
    "        self.loss = nn.SmoothL1Loss() # Huber loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#        x = self.dropout(x) # dropout for regularization\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                max_mem_size=100000, eps_end=0.01, eps_dec=1e-5, weight_decay=1e-5):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)] # list of actions\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions=n_actions, input_dims=input_dims, \n",
    "                                  fc1_dims=32, fc2_dims=32, weight_decay=weight_decay)\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype= np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size # residual. The memory is finite, so we are reusing\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] =  action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device) # [] is used because of the nn library. torch.Size([1, 6])\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item() # .item() to get integer\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size) \n",
    "        # select samples the number of self.batch_size out of max_mem \n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False) # Don't select the same thing again\n",
    "        # array slicing [0,1,2,...,self.batch_size-1]\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32) \n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "                                                 \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golde\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  8\n",
      "Size of Action Space ->  3\n",
      "average score 6.19 epsilon 0.00  profit_iteration 0.01  iterations 462.00  long_return 1.04 \n"
     ]
    }
   ],
   "source": [
    "from env import TradingSPYEnv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env = TradingSPYEnv(mode='test', init_invest=100.0, sma_len=[5,25,50,100])\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.n\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "agent = Agent(gamma=0.99, epsilon=0.0, batch_size=64, n_actions=num_actions, eps_end=0.01, input_dims = [num_states], lr=0.003,\n",
    "              weight_decay=1e-5)\n",
    "state_dict_path = os.path.join(os.getcwd(),'test-huber.pth')\n",
    "agent.Q_eval.load_state_dict(T.load(state_dict_path))\n",
    "\n",
    "scores, eps_history = [], []\n",
    "n_games = 1000\n",
    "\n",
    "\n",
    "score = 0\n",
    "done = False\n",
    "observation = env._get_observation()\n",
    "if observation.dtype == np.float64:\n",
    "    observation = observation.astype(np.float32)\n",
    "while not done:\n",
    "    action = agent.choose_action(observation)\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    if (observation_ is not None): \n",
    "        if (observation_.dtype == np.float64):\n",
    "            observation_ = observation_.astype(np.float32)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "#        agent.learn()\n",
    "#        observation = observation_\n",
    "scores.append(score)\n",
    "eps_history.append(agent.epsilon)\n",
    "\n",
    "avg_score = np.mean(scores[-100:])\n",
    "\n",
    "print('average score %.2f' % avg_score,\n",
    "     'epsilon %.2f ' % agent.epsilon,\n",
    "     'profit_iteration %.2f ' % info['profit_iteration'],\n",
    "     'iterations %.2f ' % info['iterations'],\n",
    "     'long_return %.2f ' % info['long_return']\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golde\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TradingSPYEnv(mode='test', init_invest=100.0, sma_len=[5,25,50,100])\n",
    "env.current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b7bc18d188>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tmp = agent.action_memory[0:462]\n",
    "plt.scatter(np.arange(len(tmp)),tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.features.shape[0]*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = env.stock_price_history[['Date','Close']].loc[4161:4623]\n",
    "df.set_index('Date').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = env.features[['Date','Close']].loc[4161:4623]\n",
    "df2.set_index('Date').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
