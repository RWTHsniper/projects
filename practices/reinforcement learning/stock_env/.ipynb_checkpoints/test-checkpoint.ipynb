{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    For a given observation, compute Q values for each action\n",
    "        Attributes\n",
    "    ----------\n",
    "    lr: float\n",
    "        learning rate\n",
    "    input_dims : list\n",
    "        [8]\n",
    "    fc1_dims : int\n",
    "        fully-connected layer 1\n",
    "    fc2_dims : int\n",
    "        fully-connected layer 2\n",
    "    n_actions : int\n",
    "        the number of actions\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions, weight_decay=1e-5,p=0.5):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "#        self.dropout = nn.Dropout(p) \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#        self.loss = nn.MSELoss()\n",
    "        self.loss = nn.SmoothL1Loss() # Huber loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#        x = self.dropout(x) # dropout for regularization\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                max_mem_size=100000, eps_end=0.01, eps_dec=1e-5, weight_decay=1e-5):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)] # list of actions\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions=n_actions, input_dims=input_dims, \n",
    "                                  fc1_dims=32, fc2_dims=32, weight_decay=weight_decay)\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype= np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size # residual. The memory is finite, so we are reusing\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] =  action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device) # [] is used because of the nn library. torch.Size([1, 6])\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item() # .item() to get integer\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size) \n",
    "        # select samples the number of self.batch_size out of max_mem \n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False) # Don't select the same thing again\n",
    "        # array slicing [0,1,2,...,self.batch_size-1]\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32) \n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "                                                 \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golde\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  8\n",
      "Size of Action Space ->  3\n",
      "average score 6.19 epsilon 0.00  profit_iteration 0.01  iterations 462.00  long_return 1.04 \n"
     ]
    }
   ],
   "source": [
    "from env import TradingSPYEnv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env = TradingSPYEnv(mode='test', init_invest=100.0, sma_len=[5,25,50,100])\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.n\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "agent = Agent(gamma=0.99, epsilon=0.0, batch_size=64, n_actions=num_actions, eps_end=0.01, input_dims = [num_states], lr=0.003,\n",
    "              weight_decay=1e-5)\n",
    "state_dict_path = os.path.join(os.getcwd(),'test-huber.pth')\n",
    "agent.Q_eval.load_state_dict(T.load(state_dict_path))\n",
    "\n",
    "scores, eps_history = [], []\n",
    "n_games = 1000\n",
    "\n",
    "\n",
    "score = 0\n",
    "done = False\n",
    "observation = env._get_observation()\n",
    "if observation.dtype == np.float64:\n",
    "    observation = observation.astype(np.float32)\n",
    "while not done:\n",
    "    action = agent.choose_action(observation)\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    if (observation_ is not None): \n",
    "        if (observation_.dtype == np.float64):\n",
    "            observation_ = observation_.astype(np.float32)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "#        agent.learn()\n",
    "#        observation = observation_\n",
    "scores.append(score)\n",
    "eps_history.append(agent.epsilon)\n",
    "\n",
    "avg_score = np.mean(scores[-100:])\n",
    "\n",
    "print('average score %.2f' % avg_score,\n",
    "     'epsilon %.2f ' % agent.epsilon,\n",
    "     'profit_iteration %.2f ' % info['profit_iteration'],\n",
    "     'iterations %.2f ' % info['iterations'],\n",
    "     'long_return %.2f ' % info['long_return']\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golde\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TradingSPYEnv(mode='test', init_invest=100.0, sma_len=[5,25,50,100])\n",
    "env.current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b7bb8f1e48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAATCElEQVR4nO3df6zdd33f8edrTtyONCMEX0IWG2wka6o7kZAdecmCIGlHZqdQqxJ/2KL8GsiiS6S1mzo5qwTa9g8bUzXapbhW6mVsTdK1JGDRBAe109IVhfqYBeKQGFyTLnfO5gupAoVKqdv3/jhfi8PlXN+vr8+9176f50M6Ot/v58c5n4+dvPy9n/M995OqQpLUjr+x2gOQJK0sg1+SGmPwS1JjDH5JaozBL0mNuWy1BzDJhg0bavPmzas9DEm6ZBw9evSbVTXTp+1FGfybN29mOByu9jAk6ZKR5E/7tnWpR5IaY/BLUmMMfklqjMEvSY0x+CWpMYve1ZNkE/BJ4LXAXwMHqurj89oE+DhwB/A94H1V9aWubkdXtw64t6o+OtUZdDbv+72J5c999KeX4+0k6ZLV54r/DPDPq+rHgZuAO5Nsm9dmJ7C1e+wFPgGQZB1wT1e/Ddgzoe8FWyj0F6uTpBYtGvxV9cLZq/eq+g7wDHDdvGa7gE/WyBPAVUmuBbYDJ6rqZFW9DDzYtZUkrZLzWuNPshl4E/DFeVXXAc+Pnc92ZQuVT3rtvUmGSYZzc3PnMyxJ0nnoHfxJfgz4FPALVfXt+dUTutQ5yn+4sOpAVQ2qajAz0+tbx5KkJej1KxuSXM4o9H+rqh6a0GQW2DR2vhE4BaxfoFyStEoWveLv7tj5TeCZqvqVBZodAt6TkZuAl6rqBeAIsDXJliTrgd1d26k615073tUjST+ozxX/LcC7gaeSPNmV/UvgdQBVtR94hNGtnCcY3c75/q7uTJK7gMOMbuc8WFVPT3MCZxnwktTPosFfVf+TyWv1420KuHOBukcY/cMgSboI+M1dSWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjFt2IJclB4O3A6ar6uxPqfwl419jr/TgwU1UvJnkO+A7wV8CZqhpMa+CSpKXpc8V/H7Bjocqq+lhV3VBVNwB3A/+jql4ca3JbV2/oS9JFYNHgr6rHgRcXa9fZAzxwQSOSJC2rqa3xJ3kFo58MPjVWXMBjSY4m2btI/71JhkmGc3Nz0xqWJGmeaX64+w7gj+Yt89xSVTcCO4E7k7xloc5VdaCqBlU1mJmZmeKwJEnjphn8u5m3zFNVp7rn08DDwPYpvp8kaQmmEvxJXgm8FfjMWNkVSa48ewzcDhybxvtJkpauz+2cDwC3AhuSzAIfAS4HqKr9XbOfBR6rqu+Odb0GeDjJ2fe5v6o+N72hS5KWYtHgr6o9Pdrcx+i2z/Gyk8D1Sx2YJGl5+M1dSWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjFg3+JAeTnE4ycdvEJLcmeSnJk93jw2N1O5IcT3Iiyb5pDlyStDR9rvjvA3Ys0uYPq+qG7vGvAZKsA+4BdgLbgD1Jtl3IYCVJF27R4K+qx4EXl/Da24ETVXWyql4GHgR2LeF1JElTNK01/puTfDnJo0l+oiu7Dnh+rM1sVzZRkr1JhkmGc3NzUxqWJGm+aQT/l4DXV9X1wK8Bn+7KM6FtLfQiVXWgqgZVNZiZmZnCsCRJk1xw8FfVt6vqz7vjR4DLk2xgdIW/aazpRuDUhb6fJOnCXHDwJ3ltknTH27vX/BZwBNiaZEuS9cBu4NCFvp8k6cJctliDJA8AtwIbkswCHwEuB6iq/cA7gZ9Pcgb4C2B3VRVwJsldwGFgHXCwqp5elllIknrLKKMvLoPBoIbD4WoPQ5IuGUmOVtWgT1u/uStJjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjFg3+JAeTnE5ybIH6dyX5Svf4QpLrx+qeS/JUkieT+Av2Jeki0OeK/z5gxznqvwG8tareCPwb4MC8+tuq6oa+GwRIkpbXolsvVtXjSTafo/4LY6dPMNpUXZJ0kZr2Gv8HgEfHzgt4LMnRJHvP1THJ3iTDJMO5ubkpD0uSdNaiV/x9JbmNUfC/eaz4lqo6leQ1wOeTPFtVj0/qX1UH6JaJBoPBxbcRsCStEVO54k/yRuBeYFdVfetseVWd6p5PAw8D26fxfpKkpbvg4E/yOuAh4N1V9bWx8iuSXHn2GLgdmHhnkCRp5Sy61JPkAeBWYEOSWeAjwOUAVbUf+DDwauDXkwCc6e7guQZ4uCu7DLi/qj63DHOQJJ2HPnf17Fmk/oPAByeUnwSu/+EekqTV5Dd3JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNWTT4kxxMcjrJxG0TM/KrSU4k+UqSG8fqdiQ53tXtm+bAJUlL0+eK/z5gxznqdwJbu8de4BMASdYB93T124A9SbZdyGAlSRdu0eCvqseBF8/RZBfwyRp5ArgqybXAduBEVZ2sqpeBB7u2kqRVNI01/uuA58fOZ7uyhconSrI3yTDJcG5ubgrDkiRNMo3gz4SyOkf5RFV1oKoGVTWYmZmZwrAkSZNcNoXXmAU2jZ1vBE4B6xcolyStomlc8R8C3tPd3XMT8FJVvQAcAbYm2ZJkPbC7aytJWkWLXvEneQC4FdiQZBb4CHA5QFXtBx4B7gBOAN8D3t/VnUlyF3AYWAccrKqnl2EOkqTzsGjwV9WeReoLuHOBukcY/cMgSbpI+M1dSWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjegV/kh1Jjic5kWTfhPpfSvJk9ziW5K+SXN3VPZfkqa5uOO0JSJLOT5+tF9cB9wBvY7Sx+pEkh6rqq2fbVNXHgI917d8B/GJVvTj2MrdV1TenOnJJ0pL0ueLfDpyoqpNV9TLwILDrHO33AA9MY3CSpOnrE/zXAc+Pnc92ZT8kySuAHcCnxooLeCzJ0SR7F3qTJHuTDJMM5+bmegxLkrQUfYI/E8pqgbbvAP5o3jLPLVV1I7ATuDPJWyZ1rKoDVTWoqsHMzEyPYUmSlqJP8M8Cm8bONwKnFmi7m3nLPFV1qns+DTzMaOlIkrRK+gT/EWBrki1J1jMK90PzGyV5JfBW4DNjZVckufLsMXA7cGwaA5ckLc2id/VU1ZkkdwGHgXXAwap6OsmHuvr9XdOfBR6rqu+Odb8GeDjJ2fe6v6o+N80JSJLOT6oWWq5fPYPBoIZDb/mXpL6SHK2qQZ+2fnNXkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4JekxvQK/iQ7khxPciLJvgn1tyZ5KcmT3ePDfftKklbWojtwJVkH3AO8jdH+u0eSHKqqr85r+odV9fYl9pUkrZA+V/zbgRNVdbKqXgYeBHb1fP0L6StJWgZ9gv864Pmx89mubL6bk3w5yaNJfuI8+5Jkb5JhkuHc3FyPYUmSlqJP8GdC2fyNer8EvL6qrgd+Dfj0efQdFVYdqKpBVQ1mZmZ6DEuStBR9gn8W2DR2vhE4Nd6gqr5dVX/eHT8CXJ5kQ5++kqSV1Sf4jwBbk2xJsh7YDRwab5DktUnSHW/vXvdbffpKklbWonf1VNWZJHcBh4F1wMGqejrJh7r6/cA7gZ9Pcgb4C2B3VRUwse8yzUWS1ENG+XxxGQwGNRwOV3sYknTJSHK0qgZ92vrNXUlqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSY3oFf5IdSY4nOZFk34T6dyX5Svf4QpLrx+qeS/JUkieTuLuKJK2yRbdeTLIOuAd4G6PN048kOVRVXx1r9g3grVX1Z0l2AgeAvz9Wf1tVfXOK45YkLVGfK/7twImqOllVLwMPArvGG1TVF6rqz7rTJ4CN0x2mJGla+gT/dcDzY+ezXdlCPgA8OnZewGNJjibZu1CnJHuTDJMM5+bmegxLkrQUiy71AJlQNnGH9iS3MQr+N48V31JVp5K8Bvh8kmer6vEfesGqA4yWiBgMBhffDvCStEb0ueKfBTaNnW8ETs1vlOSNwL3Arqr61tnyqjrVPZ8GHma0dCRJWiV9gv8IsDXJliTrgd3AofEGSV4HPAS8u6q+NlZ+RZIrzx4DtwPHpjV4SdL5W3Spp6rOJLkLOAysAw5W1dNJPtTV7wc+DLwa+PUkAGeqagBcAzzclV0G3F9Vn1uWmUiSeknVxbecPhgMajj0ln9J6ivJ0e6Ce1F+c1eSGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1Jg+m62TZAfwcUY7cN1bVR+dV5+u/g7ge8D7qupLffpO0+Z9v7dcLy1JK+q5j/70sr32olf8SdYB9wA7gW3AniTb5jXbCWztHnuBT5xH36kw9CWtJcuZaX2WerYDJ6rqZFW9DDwI7JrXZhfwyRp5ArgqybU9+0qSVlCf4L8OeH7sfLYr69OmT18AkuxNMkwynJub6zEsSdJS9An+TCibv0P7Qm369B0VVh2oqkFVDWZmZnoMS5K0FH0+3J0FNo2dbwRO9WyzvkdfSdIK6nPFfwTYmmRLkvXAbuDQvDaHgPdk5Cbgpap6oWffqVjOT8AlaaUtZ6YtesVfVWeS3AUcZnRL5sGqejrJh7r6/cAjjG7lPMHods73n6vvsswEw1+S+kjVxCX3VTUYDGo4HK72MCTpkpHkaFUN+rT1m7uS1BiDX5IaY/BLUmMMfklqzEX54W6SOeBPl9h9A/DNKQ7nUuCc177W5gvO+Xy9vqp6ffv1ogz+C5Fk2PeT7bXCOa99rc0XnPNycqlHkhpj8EtSY9Zi8B9Y7QGsAue89rU2X3DOy2bNrfFLks5tLV7xS5LOweCXpMasmeBPsiPJ8SQnkuxb7fGcjySbkvz3JM8keTrJP+3Kr07y+SRf755fNdbn7m6ux5P8o7Hyv5fkqa7uV5OkK/+RJL/dlX8xyeYVn+gESdYl+V9JPtudr+k5J7kqye8mebb7+765gTn/Yvff9bEkDyT50bU25yQHk5xOcmysbEXmmOS93Xt8Pcl7ew24qi75B6Nf+fwnwBsYbf7yZWDbao/rPMZ/LXBjd3wl8DVGm9P/O2BfV74P+Lfd8bZujj8CbOnmvq6r+2PgZka7nz0K7OzK/wmwvzveDfz2as+7G8s/A+4HPtudr+k5A/8Z+GB3vB64ai3PmdFWq98A/mZ3/t+A9621OQNvAW4Ejo2VLfscgauBk93zq7rjVy063tX+H2FKf+g3A4fHzu8G7l7tcV3AfD4DvA04DlzblV0LHJ80P0b7HdzctXl2rHwP8Bvjbbrjyxh9OzCrPM+NwO8DP8n3g3/Nzhn4W4xCMPPK1/Kcz+67fXU3ns8Ct6/FOQOb+cHgX/Y5jrfp6n4D2LPYWNfKUk/vTd0vdt2PcG8CvghcU6OdzOieX9M1O9fm9rMTyn+gT1WdAV4CXr0sk+jvPwD/AvjrsbK1POc3AHPAf+qWt+5NcgVreM5V9X+Afw/8b+AFRrvzPcYanvOYlZjjkrJvrQR/703dL2ZJfgz4FPALVfXtczWdULbY5vYX1Z9RkrcDp6vqaN8uE8ouqTkzulK7EfhEVb0J+C6jJYCFXPJz7ta1dzFa0vjbwBVJfu5cXSaUXVJz7mGac1zS3NdK8PfZEP6iluRyRqH/W1X1UFf8/5Jc29VfC5zuyhea72x3PL/8B/okuQx4JfDi9GfS2y3AzyR5DngQ+Mkk/5W1PedZYLaqvtid/y6jfwjW8pz/IfCNqpqrqr8EHgL+AWt7zmetxByXlH1rJfhXbFP35dB9cv+bwDNV9StjVYeAs5/Sv5fR2v/Z8t3dJ/1bgK3AH3c/Tn4nyU3da75nXp+zr/VO4A+qWxRcDVV1d1VtrKrNjP6+/qCqfo61Pef/Czyf5O90RT8FfJU1PGdGSzw3JXlFN9afAp5hbc/5rJWY42Hg9iSv6n66ur0rO7eV/gBkGT9YuYPR3TB/Avzyao/nPMf+ZkY/nn0FeLJ73MFoDe/3ga93z1eP9fnlbq7H6T7578oHwLGu7j/y/W9n/yjwO8AJRncOvGG15z025lv5/oe7a3rOwA3AsPu7/jSjOzHW+pz/FfBsN97/wuhuljU1Z+ABRp9h/CWjq/APrNQcgX/clZ8A3t9nvP7KBklqzFpZ6pEk9WTwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMb8fxI55SZqalwbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tmp = agent.action_memory[4161:4623]\n",
    "plt.scatter(np.arange(len(tmp)),tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.features.shape[0]*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = env.stock_price_history[['Date','Close']].loc[4161:4623]\n",
    "df.set_index('Date').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = env.features[['Date','Close']].loc[4161:4623]\n",
    "df2.set_index('Date').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
