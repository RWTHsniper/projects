{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from Heston_NN import Net,weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net = True\n",
    "#save_net = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "arbitrage_weight = [1e-3,1e-3]; \n",
    "l2_weight = 1e-5\n",
    "arbitrage_weight = [0.0,0.0]; \n",
    "#l2_weight = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>T</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>kappa</th>\n",
       "      <th>theta</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.448511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5686 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        k    T        v0       rho   kappa   theta   sigma\n",
       "0    -0.2  1.0  0.316228 -1.000000   0.001   0.001   0.001\n",
       "1    -0.2  1.0  0.316228 -0.714286   0.001   0.001   0.001\n",
       "2    -0.2  1.0  0.316228 -0.428571   0.001   0.001   0.001\n",
       "3    -0.2  1.0  0.316228 -0.142857   0.001   0.001   0.001\n",
       "4    -0.2  1.0  0.316228  0.142857   0.001   0.001   0.001\n",
       "...   ...  ...       ...       ...     ...     ...     ...\n",
       "5681  0.2  1.0  0.415440  1.000000  20.000  10.000  14.286\n",
       "5682  0.2  1.0  0.448511  1.000000  20.000  10.000  14.286\n",
       "5683  0.2  1.0  0.481581  1.000000  20.000  10.000  14.286\n",
       "5684  0.2  1.0  0.514652  1.000000  20.000  10.000  14.286\n",
       "5685  0.2  1.0  0.547723  1.000000  20.000  10.000  14.286\n",
       "\n",
       "[5686 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_csv('Heston_data_input')\n",
    "X = table.drop(['C_price','delta'], axis=1)\n",
    "y = table[['C_price']]\n",
    "#X = df[['k','T','C_price']]\n",
    "#y = df[['v0','rho','kappa','theta','sigma']]\n",
    "T_loc = list(X.columns).index('T')\n",
    "k_loc = list(X.columns).index('k') # log strike \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>T</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>kappa</th>\n",
       "      <th>theta</th>\n",
       "      <th>sigma</th>\n",
       "      <th>C_price</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.0</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.431894</td>\n",
       "      <td>0.268730</td>\n",
       "      <td>6.065719</td>\n",
       "      <td>4.605333</td>\n",
       "      <td>4.011151</td>\n",
       "      <td>0.240721</td>\n",
       "      <td>0.617825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.130813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075799</td>\n",
       "      <td>0.783798</td>\n",
       "      <td>7.585582</td>\n",
       "      <td>2.863153</td>\n",
       "      <td>5.019007</td>\n",
       "      <td>0.160363</td>\n",
       "      <td>0.412394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.085714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349298</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.429429</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.042445</td>\n",
       "      <td>0.073567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.028571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415440</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4.286286</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.834684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.429000</td>\n",
       "      <td>7.143143</td>\n",
       "      <td>8.572000</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.954826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.286000</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>1.140729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 k       T           v0          rho        kappa  \\\n",
       "count  5686.000000  5686.0  5686.000000  5686.000000  5686.000000   \n",
       "mean      0.006723     1.0     0.431894     0.268730     6.065719   \n",
       "std       0.130813     0.0     0.075799     0.783798     7.585582   \n",
       "min      -0.200000     1.0     0.316228    -1.000000     0.001000   \n",
       "25%      -0.085714     1.0     0.349298    -0.428571     0.001000   \n",
       "50%       0.028571     1.0     0.415440     0.428571     0.001000   \n",
       "75%       0.142857     1.0     0.481581     1.000000    11.429000   \n",
       "max       0.200000     1.0     0.547723     1.000000    20.000000   \n",
       "\n",
       "             theta        sigma      C_price        delta  \n",
       "count  5686.000000  5686.000000  5686.000000  5686.000000  \n",
       "mean      4.605333     4.011151     0.240721     0.617825  \n",
       "std       2.863153     5.019007     0.160363     0.412394  \n",
       "min       0.001000     0.001000     0.001001     0.001640  \n",
       "25%       1.429429     0.001000     0.042445     0.073567  \n",
       "50%       4.286286     0.001000     0.289540     0.834684  \n",
       "75%       7.143143     8.572000     0.372752     0.954826  \n",
       "max      10.000000    14.286000     0.499876     1.140729  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "...     X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "...     X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "input_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "X_val_scaled = input_scaler.transform(X_val)\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "\n",
    "#X_train_scaled = torch.FloatTensor(X_train_scaled,requires_grad=True)\n",
    "#X_test_scaled = torch.FloatTensor(X_test_scaled,requires_grad=True)\n",
    "X_train_scaled = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_train_scaled.requires_grad = True\n",
    "X_val_scaled = torch.FloatTensor(X_val_scaled).to(device)\n",
    "X_val_scaled.requires_grad = True\n",
    "X_test_scaled = torch.FloatTensor(X_test_scaled).to(device)\n",
    "X_test_scaled.requires_grad = True\n",
    "\n",
    "y_train = torch.FloatTensor(y_train.values).to(device)\n",
    "y_val = torch.FloatTensor(y_val.values).to(device)\n",
    "y_test = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "if save_net:\n",
    "    dump(input_scaler, open('input_scaler.pkl', 'wb'))\n",
    "#    input_scaler = load(open('input_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5       , 1.        , 4.31975162, 0.5       , 0.0500025 ,\n",
       "       0.10001   , 0.0700035 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.00000000e-01, -1.00000000e+00, -1.36602540e+00,  5.00000000e-01,\n",
       "       -5.00025001e-05, -1.00010001e-04, -7.00035002e-05])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_neurons=128,device=device)\n",
    "model.apply(weights_init)\n",
    "#model.load_state_dict(torch.load('heston_NN_intermediate')) # load model\n",
    "#model.load_state_dict(torch.load('heston_NN_final')) # load model\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  500  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  1000  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  1500  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  2000  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  2500  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  3000  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  3500  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  4000  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  4500  \n",
      "Loss: 0.08195436000823975\n",
      "Loss: 0.08258985728025436\n",
      "Epoch:  5000  \n",
      "Loss: 0.008920109830796719\n",
      "Loss: 0.008906172588467598\n",
      "Epoch:  5500  \n",
      "Loss: 0.006620591972023249\n",
      "Loss: 0.006460275035351515\n",
      "Epoch:  6000  \n",
      "Loss: 0.005647278856486082\n",
      "Loss: 0.0054773432202637196\n",
      "Epoch:  6500  \n",
      "Loss: 0.00489420210942626\n",
      "Loss: 0.004735976457595825\n",
      "Epoch:  7000  \n",
      "Loss: 0.0042472705245018005\n",
      "Loss: 0.00410904036834836\n",
      "Epoch:  7500  \n",
      "Loss: 0.00369705050252378\n",
      "Loss: 0.003585963975638151\n",
      "Epoch:  8000  \n",
      "Loss: 0.0032813786529004574\n",
      "Loss: 0.003202545689418912\n",
      "Epoch:  8500  \n",
      "Loss: 0.0029731811955571175\n",
      "Loss: 0.002916018245741725\n",
      "Epoch:  9000  \n",
      "Loss: 0.002747985301539302\n",
      "Loss: 0.002714231377467513\n",
      "Epoch:  9500  \n",
      "Loss: 0.002561601111665368\n",
      "Loss: 0.0025474787689745426\n",
      "Epoch:  10000  \n",
      "Loss: 0.0024099552538245916\n",
      "Loss: 0.0024128227960318327\n",
      "Epoch:  10500  \n",
      "Loss: 0.0022770396899431944\n",
      "Loss: 0.002289785537868738\n",
      "Epoch:  11000  \n",
      "Loss: 0.0021654367446899414\n",
      "Loss: 0.0021869924385100603\n",
      "Epoch:  11500  \n",
      "Loss: 0.002065356122329831\n",
      "Loss: 0.002096505369991064\n",
      "Epoch:  12000  \n",
      "Loss: 0.0019624168053269386\n",
      "Loss: 0.0020006413105875254\n",
      "Epoch:  12500  \n",
      "Loss: 0.0018764105625450611\n",
      "Loss: 0.0019176382338628173\n",
      "Epoch:  13000  \n",
      "Loss: 0.0018015786772593856\n",
      "Loss: 0.0018457587575539947\n",
      "Epoch:  13500  \n",
      "Loss: 0.0017364050727337599\n",
      "Loss: 0.0017835584003478289\n",
      "Epoch:  14000  \n",
      "Loss: 0.0016834979178383946\n",
      "Loss: 0.0017310198163613677\n",
      "Epoch:  14500  \n",
      "Loss: 0.001639785710722208\n",
      "Loss: 0.0016850004903972149\n",
      "Epoch:  15000  \n",
      "Loss: 0.00160133873578161\n",
      "Loss: 0.0016444007633253932\n",
      "Epoch:  15500  \n",
      "Loss: 0.0015708190621808171\n",
      "Loss: 0.0016119321808218956\n",
      "Epoch:  16000  \n",
      "Loss: 0.0015458371490240097\n",
      "Loss: 0.0015849139308556914\n",
      "Epoch:  16500  \n",
      "Loss: 0.0015230068238452077\n",
      "Loss: 0.0015610926784574986\n",
      "Epoch:  17000  \n",
      "Loss: 0.001504847896285355\n",
      "Loss: 0.001541632111184299\n",
      "Epoch:  17500  \n",
      "Loss: 0.0014906624564900994\n",
      "Loss: 0.0015266529517248273\n",
      "Epoch:  18000  \n",
      "Loss: 0.0014778791228309274\n",
      "Loss: 0.0015135221183300018\n",
      "Epoch:  18500  \n",
      "Loss: 0.0014666238566860557\n",
      "Loss: 0.0015020854771137238\n",
      "Epoch:  19000  \n",
      "Loss: 0.0014571045758202672\n",
      "Loss: 0.001492996932938695\n",
      "Epoch:  19500  \n",
      "Loss: 0.0014490069588646293\n",
      "Loss: 0.0014849608996883035\n",
      "Epoch:  20000  \n",
      "Loss: 0.0014416612684726715\n",
      "Loss: 0.0014783631777390838\n",
      "Epoch:  20500  \n",
      "Loss: 0.0014357269974425435\n",
      "Loss: 0.001472756383009255\n",
      "Epoch:  21000  \n",
      "Loss: 0.0014314347645267844\n",
      "Loss: 0.0014686910435557365\n",
      "Epoch:  21500  \n",
      "Loss: 0.001427004812285304\n",
      "Loss: 0.0014647002099081874\n",
      "Epoch:  22000  \n",
      "Loss: 0.0014234739355742931\n",
      "Loss: 0.0014618278946727514\n",
      "Epoch:  22500  \n",
      "Loss: 0.0014199017314240336\n",
      "Loss: 0.0014586981851607561\n",
      "Epoch:  23000  \n",
      "Loss: 0.0014154702657833695\n",
      "Loss: 0.0014551172498613596\n",
      "Epoch:  23500  \n",
      "Loss: 0.0014116632519289851\n",
      "Loss: 0.001451923162676394\n",
      "Epoch:  24000  \n",
      "Loss: 0.0014091924531385303\n",
      "Loss: 0.0014499847311526537\n",
      "Epoch:  24500  \n",
      "Loss: 0.0014078289968892932\n",
      "Loss: 0.001449767267331481\n",
      "Epoch:  25000  \n",
      "Loss: 0.0014057315420359373\n",
      "Loss: 0.0014476486248895526\n",
      "Epoch:  25500  \n",
      "Loss: 0.001403669361025095\n",
      "Loss: 0.0014458097284659743\n",
      "Epoch:  26000  \n",
      "Loss: 0.001401956775225699\n",
      "Loss: 0.0014443097170442343\n",
      "Epoch:  26500  \n",
      "Loss: 0.0014003757387399673\n",
      "Loss: 0.0014428868889808655\n",
      "Epoch:  27000  \n",
      "Loss: 0.0013984092511236668\n",
      "Loss: 0.001441082451492548\n",
      "Epoch:  27500  \n",
      "Loss: 0.0013965091202408075\n",
      "Loss: 0.0014394602039828897\n",
      "Epoch:  28000  \n",
      "Loss: 0.0013946595136076212\n",
      "Loss: 0.0014380530919879675\n",
      "Epoch:  28500  \n",
      "Loss: 0.0013929294655099511\n",
      "Loss: 0.0014369264245033264\n",
      "Epoch:  29000  \n",
      "Loss: 0.0013914552982896566\n",
      "Loss: 0.001435847720131278\n",
      "Epoch:  29500  \n",
      "Loss: 0.0013904894003644586\n",
      "Loss: 0.0014358601765707135\n",
      "Epoch:  30000  \n",
      "Loss: 0.0013893371215090156\n",
      "Loss: 0.0014350638957694173\n",
      "Epoch:  30500  \n",
      "Loss: 0.0013883200008422136\n",
      "Loss: 0.001434482168406248\n",
      "Epoch:  31000  \n",
      "Loss: 0.001387066557072103\n",
      "Loss: 0.0014336173189803958\n",
      "Epoch:  31500  \n",
      "Loss: 0.0013862039195373654\n",
      "Loss: 0.001433313824236393\n",
      "Epoch:  32000  \n",
      "Loss: 0.001385154901072383\n",
      "Loss: 0.001432811259292066\n",
      "Epoch:  32500  \n",
      "Loss: 0.0013838980812579393\n",
      "Loss: 0.001431493554264307\n",
      "Epoch:  33000  \n",
      "Loss: 0.0013827955117449164\n",
      "Loss: 0.0014303119387477636\n",
      "Epoch:  33500  \n",
      "Loss: 0.0013816014397889376\n",
      "Loss: 0.0014289626851677895\n",
      "Epoch:  34000  \n",
      "Loss: 0.0013813171535730362\n",
      "Loss: 0.0014287467347458005\n",
      "Epoch:  34500  \n",
      "Loss: 0.0013804967748001218\n",
      "Loss: 0.0014278062153607607\n",
      "Epoch:  35000  \n",
      "Loss: 0.0013798926956951618\n",
      "Loss: 0.0014270995743572712\n",
      "Epoch:  35500  \n",
      "Loss: 0.0013792241225019097\n",
      "Loss: 0.0014266152866184711\n",
      "Epoch:  36000  \n",
      "Loss: 0.0013785965275019407\n",
      "Loss: 0.0014263131888583302\n",
      "Epoch:  36500  \n",
      "Loss: 0.0013779525179415941\n",
      "Loss: 0.0014254417037591338\n",
      "Epoch:  37000  \n",
      "Loss: 0.0013772344682365656\n",
      "Loss: 0.0014242237666621804\n",
      "Epoch:  37500  \n",
      "Loss: 0.0013770228251814842\n",
      "Loss: 0.0014237966388463974\n",
      "Epoch:  38000  \n",
      "Loss: 0.0013764762552455068\n",
      "Loss: 0.0014224752085283399\n",
      "Epoch:  38500  \n",
      "Loss: 0.0013756226981058717\n",
      "Loss: 0.0014213976683095098\n",
      "Epoch:  39000  \n",
      "Loss: 0.0013747401535511017\n",
      "Loss: 0.0014200159348547459\n",
      "Epoch:  39500  \n",
      "Loss: 0.0013737460831180215\n",
      "Loss: 0.001418555504642427\n",
      "Epoch:  40000  \n",
      "Loss: 0.0013722189469262958\n",
      "Loss: 0.0014162736479192972\n",
      "Epoch:  40500  \n",
      "Loss: 0.001370936050079763\n",
      "Loss: 0.001414358033798635\n",
      "Epoch:  41000  \n",
      "Loss: 0.001369347097352147\n",
      "Loss: 0.001411585370078683\n",
      "Epoch:  41500  \n",
      "Loss: 0.001367279444821179\n",
      "Loss: 0.0014084851136431098\n",
      "Epoch:  42000  \n",
      "Loss: 0.0013654137728735805\n",
      "Loss: 0.0014063345734030008\n",
      "Epoch:  42500  \n",
      "Loss: 0.0013630295870825648\n",
      "Loss: 0.0014036573702469468\n",
      "Epoch:  43000  \n",
      "Loss: 0.001361260307021439\n",
      "Loss: 0.001401637913659215\n",
      "Epoch:  43500  \n",
      "Loss: 0.0013596391072496772\n",
      "Loss: 0.001399726839736104\n",
      "Epoch:  44000  \n",
      "Loss: 0.0013577128993347287\n",
      "Loss: 0.0013971063308417797\n",
      "Epoch:  44500  \n",
      "Loss: 0.0013558042701333761\n",
      "Loss: 0.0013949053827673197\n",
      "Epoch:  45000  \n",
      "Loss: 0.00135372846852988\n",
      "Loss: 0.0013921679928898811\n",
      "Epoch:  45500  \n",
      "Loss: 0.0013524575624614954\n",
      "Loss: 0.001390928984619677\n",
      "Epoch:  46000  \n",
      "Loss: 0.0013502894435077906\n",
      "Loss: 0.0013887662207707763\n",
      "Epoch:  46500  \n",
      "Loss: 0.001347878947854042\n",
      "Loss: 0.0013857368612661958\n",
      "Epoch:  47000  \n",
      "Loss: 0.001346510136500001\n",
      "Loss: 0.0013841463951393962\n",
      "Epoch:  47500  \n",
      "Loss: 0.0013443538919091225\n",
      "Loss: 0.00138100644107908\n",
      "Epoch:  48000  \n",
      "Loss: 0.001342863542959094\n",
      "Loss: 0.0013794421683996916\n",
      "Epoch:  48500  \n",
      "Loss: 0.001341706607490778\n",
      "Loss: 0.0013786036288365722\n",
      "Epoch:  49000  \n",
      "Loss: 0.0013405586360022426\n",
      "Loss: 0.0013773966347798705\n",
      "Epoch:  49500  \n",
      "Loss: 0.0013397391885519028\n",
      "Loss: 0.0013766480842605233\n",
      "Epoch:  50000  \n",
      "Loss: 0.0013387978542596102\n",
      "Loss: 0.0013756956905126572\n",
      "Epoch:  50500  \n",
      "Loss: 0.001337926252745092\n",
      "Loss: 0.0013748276978731155\n",
      "Epoch:  51000  \n",
      "Loss: 0.001337248831987381\n",
      "Loss: 0.0013742727460339665\n",
      "Epoch:  51500  \n",
      "Loss: 0.0013367373030632734\n",
      "Loss: 0.001373911858536303\n",
      "Epoch:  52000  \n",
      "Loss: 0.0013363452162593603\n",
      "Loss: 0.0013735605170950294\n",
      "Epoch:  52500  \n",
      "Loss: 0.0013358485884964466\n",
      "Loss: 0.0013730934588238597\n",
      "Epoch:  53000  \n",
      "Loss: 0.0013355547562241554\n",
      "Loss: 0.001372698461636901\n",
      "Epoch:  53500  \n",
      "Loss: 0.001335363369435072\n",
      "Loss: 0.0013726315228268504\n",
      "Epoch:  54000  \n",
      "Loss: 0.0013350856024771929\n",
      "Loss: 0.001372342579998076\n",
      "Epoch:  54500  \n",
      "Loss: 0.0013349223881959915\n",
      "Loss: 0.0013721337309107184\n",
      "Epoch:  55000  \n",
      "Loss: 0.001334833214059472\n",
      "Loss: 0.0013720971764996648\n",
      "Epoch:  55500  \n",
      "Loss: 0.0013347381027415395\n",
      "Loss: 0.0013719424605369568\n",
      "Epoch:  56000  \n",
      "Loss: 0.0013347258791327477\n",
      "Loss: 0.0013720622519031167\n",
      "Epoch:  56500  \n",
      "Loss: 0.001334531931206584\n",
      "Loss: 0.001371745951473713\n",
      "Epoch:  57000  \n",
      "Loss: 0.001334666390903294\n",
      "Loss: 0.0013719982234761119\n",
      "Epoch:  57500  \n",
      "Loss: 0.0013345411280170083\n",
      "Loss: 0.001371727790683508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  58000  \n",
      "Loss: 0.0013345370534807444\n",
      "Loss: 0.0013718499103561044\n",
      "Epoch:  58500  \n",
      "Loss: 0.0013343608006834984\n",
      "Loss: 0.001371566904708743\n",
      "Epoch:  59000  \n",
      "Loss: 0.0013344944454729557\n",
      "Loss: 0.0013717437395825982\n",
      "Epoch:  59500  \n",
      "Loss: 0.0013344797771424055\n",
      "Loss: 0.0013717421097680926\n",
      "Epoch:  60000  \n",
      "Loss: 0.001334541360847652\n",
      "Loss: 0.0013718075351789594\n",
      "Epoch:  60500  \n",
      "Loss: 0.0013345045736059546\n",
      "Loss: 0.0013717814581468701\n",
      "Epoch:  61000  \n",
      "Loss: 0.0013346115592867136\n",
      "Loss: 0.0013720174320042133\n",
      "Epoch:  61500  \n",
      "Loss: 0.0013345900224521756\n",
      "Loss: 0.0013719173148274422\n",
      "Epoch:  62000  \n",
      "Loss: 0.00133474450558424\n",
      "Loss: 0.0013722270959988236\n",
      "Epoch:  62500  \n",
      "Loss: 0.0013346896739676595\n",
      "Loss: 0.0013721247669309378\n",
      "Epoch:  63000  \n",
      "Loss: 0.0013346333289518952\n",
      "Loss: 0.0013720598071813583\n",
      "Epoch:  63500  \n",
      "Loss: 0.0013348014326766133\n",
      "Loss: 0.0013723511947318912\n",
      "Epoch:  64000  \n",
      "Loss: 0.0013347682543098927\n",
      "Loss: 0.001372243743389845\n",
      "Epoch:  64500  \n",
      "Loss: 0.0013347782660275698\n",
      "Loss: 0.0013722546864300966\n",
      "Epoch:  65000  \n",
      "Loss: 0.001334882341325283\n",
      "Loss: 0.0013724451418966055\n",
      "Epoch:  65500  \n",
      "Loss: 0.001334893866442144\n",
      "Loss: 0.001372354687191546\n",
      "Epoch:  66000  \n",
      "Loss: 0.0013350129593163729\n",
      "Loss: 0.0013725083554163575\n",
      "Epoch:  66500  \n",
      "Loss: 0.0013349611544981599\n",
      "Loss: 0.0013725311728194356\n",
      "Epoch:  67000  \n",
      "Loss: 0.0013351080706343055\n",
      "Loss: 0.0013726629549637437\n",
      "Epoch:  67500  \n",
      "Loss: 0.0013351364759728312\n",
      "Loss: 0.0013725952012464404\n",
      "Epoch:  68000  \n",
      "Loss: 0.0013352420646697283\n",
      "Loss: 0.0013727854238823056\n",
      "Epoch:  68500  \n",
      "Loss: 0.0013352509122341871\n",
      "Loss: 0.0013728338526561856\n",
      "Epoch:  69000  \n",
      "Loss: 0.0013352892128750682\n",
      "Loss: 0.0013728459598496556\n",
      "Epoch:  69500  \n",
      "Loss: 0.0013353206450119615\n",
      "Loss: 0.0013729705242440104\n",
      "Epoch:  70000  \n",
      "Loss: 0.0013354388065636158\n",
      "Loss: 0.0013730725040659308\n",
      "Epoch:  70500  \n",
      "Loss: 0.001335427281446755\n",
      "Loss: 0.0013730954378843307\n",
      "Epoch:  71000  \n",
      "Loss: 0.0013354532420635223\n",
      "Loss: 0.0013731324579566717\n",
      "Epoch:  71500  \n",
      "Loss: 0.0013355541741475463\n",
      "Loss: 0.0013731939252465963\n",
      "Epoch:  72000  \n",
      "Loss: 0.0013354952679947019\n",
      "Loss: 0.0013731204671785235\n",
      "Epoch:  72500  \n",
      "Loss: 0.0013355336850509048\n",
      "Loss: 0.0013731330400332808\n",
      "Epoch:  73000  \n",
      "Loss: 0.0013355830451473594\n",
      "Loss: 0.0013732550432905555\n",
      "Epoch:  73500  \n",
      "Loss: 0.0013357100542634726\n",
      "Loss: 0.0013734339736402035\n",
      "Epoch:  74000  \n",
      "Loss: 0.0013356328709051013\n",
      "Loss: 0.0013732569059357047\n",
      "Epoch:  74500  \n",
      "Loss: 0.001335734617896378\n",
      "Loss: 0.0013734418898820877\n",
      "Epoch:  75000  \n",
      "Loss: 0.0013357315910980105\n",
      "Loss: 0.001373468665406108\n",
      "Epoch:  75500  \n",
      "Loss: 0.001335821463726461\n",
      "Loss: 0.0013735932298004627\n",
      "Epoch:  76000  \n",
      "Loss: 0.0013357993448153138\n",
      "Loss: 0.0013735031243413687\n",
      "Epoch:  76500  \n",
      "Loss: 0.0013357845600694418\n",
      "Loss: 0.0013734307140111923\n",
      "Epoch:  77000  \n",
      "Loss: 0.001335859764367342\n",
      "Loss: 0.0013735482934862375\n",
      "Epoch:  77500  \n",
      "Loss: 0.001335926353931427\n",
      "Loss: 0.00137364084366709\n",
      "Epoch:  78000  \n",
      "Loss: 0.00133604952134192\n",
      "Loss: 0.001373880892060697\n",
      "Epoch:  78500  \n",
      "Loss: 0.001336067565716803\n",
      "Loss: 0.0013738905545324087\n",
      "Epoch:  79000  \n",
      "Loss: 0.0013360786251723766\n",
      "Loss: 0.0013739648275077343\n",
      "Epoch:  79500  \n",
      "Loss: 0.0013360085431486368\n",
      "Loss: 0.0013737762346863747\n",
      "Epoch:  80000  \n",
      "Loss: 0.00133608840405941\n",
      "Loss: 0.0013738964917138219\n",
      "Epoch:  80500  \n",
      "Loss: 0.0013361467281356454\n",
      "Loss: 0.0013739814748987556\n",
      "Epoch:  81000  \n",
      "Loss: 0.0013361497549340129\n",
      "Loss: 0.0013739829882979393\n",
      "Epoch:  81500  \n",
      "Loss: 0.0013361754827201366\n",
      "Loss: 0.0013739633141085505\n",
      "Epoch:  82000  \n",
      "Loss: 0.0013361686142161489\n",
      "Loss: 0.0013740055728703737\n",
      "Epoch:  82500  \n",
      "Loss: 0.0013362537138164043\n",
      "Loss: 0.0013741478323936462\n",
      "Epoch:  83000  \n",
      "Loss: 0.0013363310135900974\n",
      "Loss: 0.0013741806615144014\n",
      "Epoch:  83500  \n",
      "Loss: 0.0013362771132960916\n",
      "Loss: 0.0013742138398811221\n",
      "Epoch:  84000  \n",
      "Loss: 0.0013362749014049768\n",
      "Loss: 0.0013741663424298167\n",
      "Epoch:  84500  \n",
      "Loss: 0.0013364499900490046\n",
      "Loss: 0.0013744699535891414\n",
      "Epoch:  85000  \n",
      "Loss: 0.0013364244950935245\n",
      "Loss: 0.0013743732124567032\n",
      "Epoch:  85500  \n",
      "Loss: 0.0013364246115088463\n",
      "Loss: 0.0013743927702307701\n",
      "Epoch:  86000  \n",
      "Loss: 0.0013364468468353152\n",
      "Loss: 0.0013744330499321222\n",
      "Epoch:  86500  \n",
      "Loss: 0.0013364868937060237\n",
      "Loss: 0.0013744884636253119\n",
      "Epoch:  87000  \n",
      "Loss: 0.0013364276383072138\n",
      "Loss: 0.001374399522319436\n",
      "Epoch:  87500  \n",
      "Loss: 0.001336496090516448\n",
      "Loss: 0.001374493120238185\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5db7e1822a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#    val_loss = model.get_loss(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marbitrage_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marbitrage_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\projects\\practices\\quant\\Heston\\Heston_NN.py\u001b[0m in \u001b[0;36mget_loss_test\u001b[1;34m(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0ml2_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcalendar_arbi_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3) # 16\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3) # 32\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3) \n",
    "epochs = 100001\n",
    "#epochs = 2001\n",
    "loss_arr = []\n",
    "val_arr =[]\n",
    "freq = 500\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % freq == 0:\n",
    "        print('Epoch: ',i,' ')\n",
    "        show_log = True\n",
    "        if save_net: \n",
    "            torch.save(model.state_dict(), \"heston_NN_intermediate\") # save the model\n",
    "    else:\n",
    "        show_log = False\n",
    " \n",
    "#    loss = model.get_loss(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "#    val_loss = model.get_loss(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    loss = model.get_loss_test(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    val_loss = model.get_loss_test(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_arr.append(float(loss))\n",
    "    val_arr.append(float(val_loss))\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhAUlEQVR4nO3deZwdZZ3v8c+vztZbls4CdBIgiUQICSEJDQRZDAYR0IgLo3Eb0YFccXf0NeIKzNVR73gZXqjo6IhzVQJiUAEHxC0ojoIkDoZsLIGEdGK2hnSS3s5Sz/2jqjtNUh066dNdp+t8369X2+fUdn71BL9V/Zynqsw5h4iIJJ8XdwEiIjI8FPgiIlVCgS8iUiUU+CIiVUKBLyJSJRT4IiJVQoEv0oeZbTKzi+KuQ2QoKPBFRKqEAl9EpEoo8EUimFnOzG4ys23hz01mlgvnTTCzn5vZHjN73sweMjMvnPdJM9tqZvvM7AkzWxTvnogckI67AJEK9RlgATAXcMDdwGeBzwEfB1qAieGyCwBnZicDHwTOdM5tM7OpQGp4yxbpn87wRaK9A/hn59xO59wu4AbgXeG8AtAEnOicKzjnHnLBTalKQA441cwyzrlNzrmNsVQvEkGBLxJtErC5z/vN4TSAfwWeBn5pZs+Y2bUAzrmngY8C1wM7zewOM5uESIVQ4ItE2wac2Of9CeE0nHP7nHMfd85NBxYD/9jTV++cW+acOy9c1wFfGd6yRfqnwBeJdjvwWTObaGYTgM8DPwQws9eZ2UlmZsBegq6ckpmdbGavCr/c7QI6w3kiFUGBLxLtC8BKYDXwOPCXcBrADODXwH7gT8AtzrkHCfrvvwzsBrYDxwCfHtaqRQ7D9AAUEZHqoDN8EZEqocAXEakSwxb4ZjbdzL5rZsuH6zNFROSAAQW+md1qZjvNbM1B0y8JLx9/umcscn+cc8845/5hMMWKiMjRG+itFf4T+Drw/Z4JZpYCvgG8muAy80fN7B6CS8m/dND673XO7TzS4iZMmOCmTp16pKuJiFStVatW7XbOTYyaN6DAd879PrwvSF9nAU87554BMLM7gMudc18CXne0xZrZUmApwAknnMDKlSuPdlMiIlXHzDb3N28wffiTgS193reE0/orYryZfQuYZ2af6m8559y3nXPNzrnmiRMjD1IiInIUBnO3TIuY1u+gfudcK/C+QXyeiIgMwmDO8FuA4/u8n0J4rxEREak8gznDfxSYYWbTgK3AEuDtZalKRBKnUCjQ0tJCV1dX3KUkQk1NDVOmTCGTyQx4nQEFvpndDiwEJphZC3Cdc+67ZvZB4AGCkTm3OufWHnnZkZ+3GFh80kknlWNzIlIBWlpaGDVqFFOnTiW475wcLeccra2ttLS0MG3atAGvN9BROm/rZ/p9wH0D/rQBcs7dC9zb3Nx8dbm3LSLx6OrqUtiXiZkxfvx4du3adUTr6dYKIjJsFPblczRtmchn2u5ZcTN0tAKGYWDhD2EjmQVDjMwONJoZ4IWLetTOuRybMCOmPRARKb9EBn7bQ//OiX7LoLbx+PrHOO0DPyxTRSIStz179rBs2TLe//73H9F6l112GcuWLWPs2LFDU9gwSmTgP3nFb1nVVcC54MIA5/vBBQLO4XDBb+fC+T64ngsIgnkLf3UZ3XmNJBBJkj179nDLLbccEvilUolUKtXvevfdV/avKWNTkYE/2FE6rz712EF9fsuvU5geDCOSKNdeey0bN25k7ty5ZDIZGhoaaGpq4rHHHmPdunW84Q1vYMuWLXR1dfGRj3yEpUuXAjB16lRWrlzJ/v37ufTSSznvvPP44x//yOTJk7n77rupra2Nec8GriIDP/5ROsZhLhoWkUG64d61rNu2t6zbPHXSaK5bPKvf+V/+8pdZs2YNjz32GA8++CCvfe1rWbNmTe+wxltvvZVx48bR2dnJmWeeyZvf/GbGjx//om089dRT3H777XznO9/hLW95C3fddRfvfOc7y7ofQ6kiAz9uPgY6wxdJtLPOOutFY9hvvvlmfvrTnwKwZcsWnnrqqUMCf9q0acydOxeAM844g02bNg1XuWWhwI9k4Py4ixBJrMOdiQ+X+vr63tcPPvggv/71r/nTn/5EXV0dCxcujLwiOJfL9b5OpVJ0dnYOS63lonH4EZwZpi4dkUQZNWoU+/bti5zX1tZGY2MjdXV1bNiwgYcffniYqxseOsMXkaowfvx4zj33XGbPnk1tbS3HHntgcMcll1zCt771LebMmcPJJ5/MggULYqx06FRk4Md9Lx0fD31pK5I8y5Yti5yey+W4//77I+f19NNPmDCBNWsOPOX1E5/4RNnrG2oV2aXjnLvXObd0zJgxMVVgmPrwRSRhKjLw4+ZAffgikjgK/EgalikiyaPAj+CbmkVEkkfJFkl9+CKSPAr8CK7P/4qIJIUCP5IuvBKpdg0NDQBs27aNK664InKZhQsXsnLlysNu56abbqKjo6P3/WWXXcaePXvKVueRqMjAN7PFZvbttra2WD7foafyiEhg0qRJLF++/KjXPzjw77vvvtjurV+RgR/3OHxnGqUjkjSf/OQnueWWW3rfX3/99dxwww0sWrSI+fPnc9ppp3H33Xcfst6mTZuYPXs2AJ2dnSxZsoQ5c+bw1re+9UX30rnmmmtobm5m1qxZXHfddUBwQ7Zt27Zx4YUXcuGFFwLB7ZZ3794NwI033sjs2bOZPXs2N910U+/nzZw5k6uvvppZs2Zx8cUXl+2ePRV5pW38DENf2ooMmfuvhe2Pl3ebx50Gl36539lLlizhox/9aO8DUO68805+8Ytf8LGPfYzRo0eze/duFixYwOtf//p+nxf7zW9+k7q6OlavXs3q1auZP39+77wvfvGLjBs3jlKpxKJFi1i9ejUf/vCHufHGG1mxYgUTJkx40bZWrVrF9773PR555BGcc5x99tm88pWvpLGxcchuw1yRZ/hxc+rDF0mcefPmsXPnTrZt28Zf//pXGhsbaWpq4tOf/jRz5szhoosuYuvWrezYsaPfbfz+97/vDd45c+YwZ86c3nl33nkn8+fPZ968eaxdu5Z169Ydtp4//OEPvPGNb6S+vp6Ghgbe9KY38dBDDwFDdxtmneFHUpeOyJA6zJn4ULriiitYvnw527dvZ8mSJdx2223s2rWLVatWkclkmDp1auRtkfuKOvt/9tln+epXv8qjjz5KY2MjV1555Utuxx0mY4bqNsw6w4/g+vlzTkRGtiVLlnDHHXewfPlyrrjiCtra2jjmmGPIZDKsWLGCzZs3H3b9Cy64gNtuuw2ANWvWsHr1agD27t1LfX09Y8aMYceOHS+6EVt/t2W+4IIL+NnPfkZHRwft7e389Kc/5fzzzy/j3h5KZ/gRnPrwRRJp1qxZ7Nu3j8mTJ9PU1MQ73vEOFi9eTHNzM3PnzuWUU0457PrXXHMN73nPe5gzZw5z587lrLPOAuD0009n3rx5zJo1i+nTp3Puuef2rrN06VIuvfRSmpqaWLFiRe/0+fPnc+WVV/Zu46qrrmLevHlD+hQtO9yfFXFrbm52LzXGdSis++K54Hmc+qmHhv2zRZJq/fr1zJw5M+4yEiWqTc1slXOuOWr5iuzSiX8cPrrQVkQSpyIDP/5x+J5G6YhI4lRk4MdPffgiQ6GSu5BHmqNpSwV+BKdhmSJlV1NTQ2trq0K/DJxztLa2UlNTc0TraZSOiAyLKVOm0NLSwq5du+IuJRFqamqYMmXKEa2jwI+kK21Fyi2TyTBt2rS4y6hq6tKJ4MxDw3REJGkU+BEcYOpnFJGEUeBHUpeOiCSPAj+KGerSEZGkUeBH8NGFVyKSPBUZ+HHfWkFdOiKSRBUZ+HHfWgHTl7YikjwVGfjxUx++iCSPAj+Cbp4mIkmkwI+gZ9qKSBIp8PujPnwRSRgFfoTgDF9EJFkU+FF04ZWIJJACP5K+tBWR5FHgR3CmL21FJHkU+P1Q4ItI0ijwI6kPX0SSR4EfwZmnWyuISOIo8COpD19EkqciAz/uu2U61IcvIslTkYEf/90yPV14JSKJU5GBHztdeCUiCaTAj2R4+HEXISJSVgr8CE4dOiKSQAr8SKZhmSKSOAr8KKZROiKSPAr8CE7NIiIJpGSLYvrSVkSSR4EfRcMyRSSBFPiR9MQrEUkeBX4Eh6cuHRFJHAV+FE+BLyLJo8CP4mVIU4q7ChGRslLgR3BemrQrxl2GiEhZKfCjpDJkrAS62lZEEkSBH8F5meCFr24dEUkOBX4E81IA+KVCzJWIiJSPAj9KKjjDLxS6Yy5ERKR8KjLw437EIWGXTrGgM3wRSY6KDPy4H3Fo4Rl+qZCP5fNFRIZCRQZ+3HoCv1hU4ItIcijwo+gMX0QSSIEfofcMX4EvIgmiwI/gpcMz/KK+tBWR5FDgR+j90lZ9+CKSIAr8CJ768EUkgRT4ESydBcDpSlsRSRAFfgR16YhIEinwI6TCM3xfX9qKSIIo8CN46TSgm6eJSLIo8COkwmGZOsMXkSRR4EfwUmGXTkl9+CKSHAr8CKlMzxm+HnMoIsmhwI+Q6h2WqTN8EUkOBX6E3sBXH76IJIgCP0I609OHr8AXkeRQ4EfouXkaCnwRSRAFfoRUeIbvfAW+iCSHAj9COuzD1xm+iCSJAj9CTx8+JQ3LFJHkUOBHyGRzgO6WKSLJosCPkE2nKTlTH76IJIoCP4LnGXkyoNsji0iCKPD7kSeDFbviLkNEpGwU+P3IWxYrdcddhohI2Sjw+5G3LJ4CX0QSRIHfj4ICX0QSRoHfj4Jl8XwFvogkx7AFvpm9wcy+Y2Z3m9nFw/W5R6toWVK+RumISHIMKPDN7FYz22lmaw6afomZPWFmT5vZtYfbhnPuZ865q4ErgbcedcXDpOjlSPsapSMiyZEe4HL/CXwd+H7PBDNLAd8AXg20AI+a2T1ACvjSQeu/1zm3M3z92XC9ilbystQW98ddhohI2Qwo8J1zvzezqQdNPgt42jn3DICZ3QFc7pz7EvC6g7dhZgZ8GbjfOfeX/j7LzJYCSwFOOOGEgZQ3JEpejrS6dEQkQQbThz8Z2NLnfUs4rT8fAi4CrjCz9/W3kHPu2865Zudc88SJEwdR3uD4qSwZp8AXkeQYaJdOFIuY5vpb2Dl3M3DzID5vWPmpHBmne+mISHIM5gy/BTi+z/spwLbBlVM5/FQNWXSGLyLJMZjAfxSYYWbTzCwLLAHuKU9Z8XOpnAJfRBJloMMybwf+BJxsZi1m9g/OuSLwQeABYD1wp3NubTmKMrPFZvbttra2cmzuqHjZGnIuj1/yY6tBRKScBjpK5239TL8PuK+sFQXbvRe4t7m5+epyb3ugvFwDKXPs7djH6FFj4ipDRKRsdGuFfnh14wDY//yumCsRESkPBX4/Mg1h4Le1xlyJiEh5KPD7kW0YD0DX3t0xVyIiUh4K/H7UjQ0u+lLgi0hSVGTgV8IonfETjgWgo019+CKSDBUZ+M65e51zS8eMiW90TMP4SfgYfltiriUTkSpXkYFfEdI5XrBGMvu3xl2JiEhZKPAPY0/2GOq7tsddhohIWSjwD6O7bhKNhR34fr/3hBMRGTEU+IdhY09gErvZ+rwehCIiI19FBn4ljNIBqGt6OTkrsGXThljrEBEph4oM/EoYpQMwfuocANo2l+WecCIisarIwK8UDVNmAVDYoTN8ERn5FPiHUzeONm8s2ReeirsSEZFBU+C/hL2jXsZx3c+yr0uPOxSRkU2B/1Ka5jLTnmPNZt1iQURGNgX+Sxg34+xgpM4Tq+IuRURkUBT4L6F++tkAdG9+NOZKREQGpyIDv1LG4QMw9kTaU2MY1bpaV9yKyIhWkYFfKePwATBj34TTmeU/yfrte+OuRkTkqFVk4FeaupPOZ4a3lcc2PB13KSIiR02BPwCjT34lAHs3/C7mSkREjp4CfyAmzSNvOcbs/LP68UVkxFLgD0Q6y57x85jnr1U/voiMWAr8AaqZcQEn2xb+8sSzcZciInJUFPgDNPrkhXjmaFuvfnwRGZkU+AM1+QwKlmX0jkcolvy4qxEROWIVGfgVdeFVj0wNbePncoZbw5pt6scXkZGnIgO/oi686qP25Rcy055jlcbji8gIVJGBX6nqT3kVnjn2aTy+iIxACvwjMWk+ea+GCbseprtYirsaEZEjosA/Eukse485k7NYy/88tyfuakREjogC/wjVn3IhL/e28tg6PedWREYWBf4Rqp1xIQCdT6ofX0RGFgX+kWo6na5UA00vPEp7dzHuakREBkyBf6S8FO1NZ7PA1vLnTc/HXY2IyIAp8I/CqFMWMdXbwdp1a+MuRURkwCoy8CvySts+sjMWApB/Wv34IjJyVGTgV+qVtr0mzqQj08iJe1eypyMfdzUiIgNSkYFf8TyPrsnncI63loc37o67GhGRAVHgH6XRMxcxyZ5nw7q/xl2KiMiAKPCPUvplC4MXzzwYZxkiIgOmwD9a419GW+0U5nQ8zN/aOuOuRkTkJSnwj5YZbsZrONdby4Or9dhDEal8CvxBGHP6YnJWYNtjv4y7FBGRl6TAHwQ78Vy6UvWcsPO3tHUU4i5HROSwFPiDkc7SPu0SXuM9yoPrnou7GhGRw1LgD1Lj2e9gtHXQ8ud74y5FROSwFPiD5E1/Je3pRqZvv4/d+7vjLkdEpF8K/MFKpSnMfAOvsr9w/6N6KIqIVC4FfhmMfcV7yFmB9j//IO5SRET6VZGBX+l3yzxE0+nsHHs6F7ffy+NbXoi7GhGRSBUZ+BV/t8wIDedfw3RvOw898OO4SxERiVSRgT8S1Z3+ZvZnxnHac99ny/MdcZcjInIIBX65pLP453yY873H+eX9P4m7GhGRQyjwy2j0+e9jb3o8c578Gn/bo7N8EaksCvxyytRSOu8TnGkbuP/u2+KuRkTkRRT4ZdZ43lU8n53EK5+5kWe2t8ZdjohILwV+uaWzeIv/jZfZNjbceUPc1YiI9FLgD4Gxp13CugmXsKj1Ntb+9ZG4yxERART4Q+bEt99Eh9WRvecaCnndY0dE4qfAHyL145rY/Ip/YUZpI6uXfSbuckREFPhDae7F7+K/Gy7m9Ge/y471/x13OSJS5RT4Q2zau77OLhpxdy3F5dvjLkdEqpgCf4hNOvZY/jL/Xziu2MLG738InIu7JBGpUgr8YXDp4rdy96i3cVLLXWz/zdfiLkdEqpQCfxh4nvGKq/+N39mZTPzDdXQ98eu4SxKRKqTAHyYTR9eSe8t/8JQ/Gf9H74bWjXGXJCJVRoE/jBbMnMofzvwanSVj761vhs49cZckIlVEgT/M3vPahXxtwuep2f8c+5ddCX4p7pJEpEoo8IdZyjM+cOW7uTF9FQ1bVtD1wPVxlyQiVaIiA3/EPdP2CE0clePVf38tt5UuouaRm/EfuyPukkSkClRk4I/EZ9oeqTNOHEfp4i/xsD8Td/f7YcN/xV2SiCRcRQZ+tXjXeTP4yclf5fHSVPw73w1P/SrukkQkwRT4MTIzrv+7BfzvsV/gCX8K/h3vgI0r4i5LRBJKgR+zumyam65cyEcz1/FM6VjcbW+BNXfFXZaIJJACvwIcP66Ob1x9EVfZ9ax2L4Pl74Xf/Sv4ftyliUiCKPArxEnHjOLrV13E/7LPcZ9dACu+AMveAh3Px12aiCSEAr+CzJ48hh++7wK+UvsxPl98L6WND+JuWQAb7ou7NBFJAAV+hTnpmFHc86Hz2Tbj7by+6wa25uvhjrcF3TwvbI67PBEZwRT4FWhMbYZvv6uZxZdcwmva/5lv8XcU1/0c97Uz4L8+Djs3xF2iiIxA5ir4gRzNzc1u5cqVcZcRq6d27OOf7lrNtuee4XOj7uWy4m/wXBFOOAfOuBJOvRwytXGXKSIVwsxWOeeaI+cp8Cufc44H1m7n//7ySVp3buNddX/k3dkHGdf1HNSMhVNfD1MvgKnnwuhJcZcrIjFS4CeE7zseeno3P/jTZn67YTtn23o+MPoPnFVYSba0P1iocRocfzY0zYGJp8CEGTB6CnjqvROpBgr8BGp5oYMfr2zhgbXbeXJ7G6faJi7IPsmiuqc5pfgE9YXWAwuncjD2hOBndBPUT4TacVA3LvgLoWYM1IwOfufC314qtn0TkaOnwE+47W1dPPJsK488+zx/2fwCT+7YR6Nr4yTbximZ7cypa2V6ejfHuV2MKbZSk38++B7gcHqCP1MLzodUNnidqYN0DWRqggNJOvzxMpDKgJcOfl7ydQZS6QOvvXT4Pup1JjgAeX2246XCnzRY+Nq88MeGp+FFKtDhAj893MVI+R03pobL507m8rmTAWjvLrJh+17W/20fG3ft555d7WxqbWfrC50UfQc4GuhkrO1nDB00eh0cl+tmQqabiekOGlNdjLN2Rnud1PoF0imPHEUy+TyZrg4y/vOk/TyenyflF/BK3ZgrYn4B80vgFzAX41XClupzgElDujY4IKSywcEqlTnwOlMLuVGQrQ8OcrlRkGsI3mfD6V4K6iYEy5oHtWODg2G6RgcXGVEU+AlUn0tzxonjOOPEcS+aXiz5tLbn2bWvm137u2ndn+eF9jwvdORp6yywtavI+s4C+7oK7O0qsnd/gb1dBboKRx7ehk9tytGQdtSlHXVpqE/71KahLu2TM5+s55PzfGq8EhnzyZpPLuXIUCJrRdL4pCmRNp+MlchQIk2JFCVSOOpKbfjpWjwcafPxAK93fokUPqmegxIl0q5AutSN54qkXAGvu5t0exup4kZShXYsvx+v0H5kO5odFfyFkxsVHkSykBsTvDeDhmMg3w5jjg8OFN37gu60XEN4QMpCoQNqG2HHWpjSfOAvlkJnsB0vDVh4cDGwoIWDhraD5vUcgA6eZlDKBwc7vxR8ZqY22Lbzg88EKHYF+wPB9EJXsE8980v5YHvFbsg2HFi+9wBv4fb6HAgP6UU46H3v/PD3/p3BAbbvNtp3Be3mecHyBx9oo3oqeqZ5XnibkojP7dlOfn94APcOXeZFdduh24GgPbx0uH4/+9fbthb8d5BrCOYVu4P9Gz0pWL+n/aZeUPbv3hT4VSSd8jh2dA3Hjq45ovW6iyX2dxXpyJfoLvp0FUp0F0t0F3y6+vzuKvh0F0p09S5z0O+CT3exRHvRZ0/Jp+Q78iVHoTt4XSj5dBd9nHMUfUfJD377ft/3Pv4Q9kJ6+NTRxZhUN+MzBRrTeSakuxib6qIm5TPZ7aSYqWcUndRZN2PZR40VyFEgQ4Harn2kujto6H4WD0j53aQK7aRKnUNXtCTTZ3eBly3rJhX48pJy6RS5hhTj4y4k5PuOkgsOAKWDDgYl31EsOXznKJQczh1YtlAKDio9yxRKPvnSgYNSd9GnM1+kMx8cwDrzwU9HocTmfJHuos8j+VLvOl0Fn/3dRdq7i+RLfuRJZg8Pnxry5MhTRzcOI20lshSoJU+aEk3Wyi4bTzb86yflGaOtg4JlMQzPHJ4FV0taeKLvmQvfO6zPdMOFP4Es3aTwyZMlTZEUPjnXzV4bRYkUKUoYPhmKdJHDYdS5TrosR5E0Hg7Dx+GRc910Ww6co4ZuOq0GP7yG03C9n+o49MT7wMH64K4wFyyPMdVtYasdR96yvSfL0/znaLHj6CZLsMcHNhx8pofvgteE2zmwZchSJG9pcAfW7LtMg+tgn9UDPmC9+9G7L+7AtoITfevdjuGoIU8nuT6feWgb1NJNJzlqyNPFgSDPkWeca2ObTcTH6/3sW5xHeeNegS8jkOcZHkamggYS9fxV0lkIDxL5Eh35Yu/BpKO7RGehRD48sJR8n3zJUfL94IBVchR8x7Q+73sOZL5zYVA6fD+IRt9ByQXp5Tv3omBxYT29tb1k8RzoITpoco+obyqsn+8vDp7adzEbwHpPRsx7sre+nji03m2XnAsPiNF1QhDjKcCLWMAwCjhqwtd9a47qPeoslKjNpMLpB2YeCGfHwZX0bKMufF9/0BIF4JiDPscbgqHUCnyRMjAzMikjk/IYXZOJuxyRSLoaR0SkSijwRUSqhAJfRKRKKPBFRKqEAl9EpEoo8EVEqoQCX0SkSijwRUSqREXfHtnMdgFH++TuCcDuMpaTBGqTaGqXQ6lNDjVS2uRE59zEqBkVHfiDYWYr+7sndLVSm0RTuxxKbXKoJLSJunRERKqEAl9EpEokOfC/HXcBFUhtEk3tcii1yaFGfJsktg9fREReLMln+CIi0ocCX0SkSiQu8M3sEjN7wsyeNrNr466n3MzseDNbYWbrzWytmX0knD7OzH5lZk+Fvxv7rPOpsD2eMLPX9Jl+hpk9Hs672cJHEZlZzsx+FE5/xMymDvuOHgUzS5nZ/5jZz8P3ahOzsWa23Mw2hP/NnFPt7WJmHwv/v7PGzG43s5qqaRPnXGJ+CJ5ithGYTvDEsb8Cp8ZdV5n3sQmYH74eRfBEuFOB/wNcG06/FvhK+PrUsB1ywLSwfVLhvD8D5xA8be1+4NJw+vuBb4WvlwA/inu/B9g2/wgsA34evlebwP8DrgpfZ4Gx1dwuwGTgWaA2fH8ncGW1tEnsBZT5H/Mc4IE+7z8FfCruuoZ4n+8GXg08ATSF05qAJ6LaAHggbKcmYEOf6W8D/r3vMuHrNMHVhRb3vr5EO0wBfgO8qk/gV3ubjA7DzQ6aXrXtEgb+FmBcWO/PgYurpU2S1qXT84/ZoyWclkjhn4rzgEeAY51zfwMIfx8TLtZfm0wOXx88/UXrOOeKQBswfkh2onxuAv6J4HnVPaq9TaYDu4DvhV1d/2Fm9VRxuzjntgJfBZ4D/ga0Oed+SZW0SdICP+qh9Ykcd2pmDcBdwEedc3sPt2jENHeY6YdbpyKZ2euAnc65VQNdJWJaotoklAbmA990zs0D2gm6K/qT+HYJ++YvJ+iemQTUm9k7D7dKxLQR2yZJC/wW4Pg+76cA22KqZciYWYYg7G9zzv0knLzDzJrC+U3AznB6f23SEr4+ePqL1jGzNDAGeL78e1I25wKvN7NNwB3Aq8zsh1R3m0BQc4tz7pHw/XKCA0A1t8tFwLPOuV3OuQLwE+AVVEmbJC3wHwVmmNk0M8sSfGFyT8w1lVU4EuC7wHrn3I19Zt0DvDt8/W6Cvv2e6UvCkQPTgBnAn8M/W/eZ2YJwm39/0Do927oC+K0LOyQrkXPuU865Kc65qQT/5r91zr2TKm4TAOfcdmCLmZ0cTloErKO62+U5YIGZ1YX7sghYT7W0SdxfIpT7B7iMYOTKRuAzcdczBPt3HsGfh6uBx8Kfywj6CH8DPBX+Htdnnc+E7fEE4UiCcHozsCac93UOXHldA/wYeJpgJML0uPf7CNpnIQe+tK36NgHmAivD/15+BjRWe7sANwAbwv35AcEInKpoE91aQUSkSiStS0dERPqhwBcRqRIKfBGRKqHAFxGpEgp8EZEqocAXEakSCnwRkSrx/wF8jN9wgSrdwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('loss')\n",
    "plt.plot(loss_arr,label='train')\n",
    "plt.plot(val_arr,label='validation')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "if save_net:     \n",
    "    X_scaled = input_scaler.transform(X)\n",
    "    X_scaled_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(table[['C_price']].values).to(device)\n",
    "    model.train()\n",
    "    for i in range(1000):\n",
    "        loss = model.get_loss_test(X_scaled_tensor, y_tensor, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    torch.save(model.state_dict(), \"heston_NN_final\") # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.381703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.381705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.381707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.381709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>0.011728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>0.011602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>0.011478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>0.011355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>0.011234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5686 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       C_price\n",
       "0     0.381700\n",
       "1     0.381703\n",
       "2     0.381705\n",
       "3     0.381707\n",
       "4     0.381709\n",
       "...        ...\n",
       "5681  0.011728\n",
       "5682  0.011602\n",
       "5683  0.011478\n",
       "5684  0.011355\n",
       "5685  0.011234\n",
       "\n",
       "[5686 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m2 = Net()\n",
    "#m2.load_state_dict(torch.load('heston_NN'))\n",
    "#2.eval() # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar  917   tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss: 0.0016289356863126159\n"
     ]
    }
   ],
   "source": [
    "loss = model.get_loss(X_test_scaled, y_test, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled[0].clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)\n",
    "dydx = grad(y,x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dydx[T_loc] < 0.0:\n",
    "    print('nono ',torch.exp(dydx[T_loc] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2 = grad(dydx[T_loc],x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled.clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_loss = torch.tensor(0.)\n",
    "butterfly_loss = torch.tensor(0.)\n",
    "for elem in X_train_scaled:\n",
    "    y= model.forward(elem)\n",
    "    dydx = grad(y, elem, create_graph = True)[0]\n",
    "    dydT = dydx[T_loc]\n",
    "    if dydT < 0.0:\n",
    "        calendar_arbi_count += 1\n",
    "        calendar_loss += torch.exp(-dydT[T_loc]) * 1e-1\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    if d2ydk2 - dydk < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_loss += torch.exp(-(d2ydk2 - dydk)) * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_input=7, num_neurons=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, num_neurons) \n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "#        x = F.relu(self.fc3(x))\n",
    "#        x = F.relu(self.fc4(x))\n",
    "#        x = F.relu(self.fc5(x))\n",
    "#        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # Penalization (loop over each data)\n",
    "        ## calendar arbitrage\n",
    "        calendar_arbi_count = 0\n",
    "        calendar_loss = torch.tensor(0.)\n",
    "        butterfly_loss = torch.tensor(0.)\n",
    "        for elem in x:\n",
    "            y= self.forward(elem)\n",
    "            dydx = grad(y, elem, create_graph = True)[0]\n",
    "            dydT = dydx[T_loc]\n",
    "            if dydT < 0.0:\n",
    "                calendar_arbi_count += 1\n",
    "                calendar_loss += torch.exp(-dydT) * arbitrage_weight[0]\n",
    "        loss += calendar_loss\n",
    "        ## butterfly arbitrage\n",
    "        butterfly_count = 0\n",
    "        dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "        d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "        # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "        # dCdK = 1/K*dcdk\n",
    "        # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "        butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "        if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "            butterfly_count += 1\n",
    "            butterfly_loss += torch.exp(-butter_ineq) * arbitrage_weight[1]\n",
    "        loss += butterfly_loss\n",
    "\n",
    "        # regularizations\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "        if show_log:\n",
    "            if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "            if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    "            print(f'Loss: {loss}')\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#        torch.nn.init.normal_(m.weight)\n",
    "#        xavier(m.weight.data)\n",
    "#        xavier(m.bias.data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "epochs = 30001\n",
    "#epochs = 101\n",
    "loss_arr = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "l2_lambda = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train_scaled)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    \n",
    "    # Penalization (loop over each data)\n",
    "    ## calendar arbitrage\n",
    "    calendar_arbi_count = 0\n",
    "    calendar_loss = torch.tensor(0.)\n",
    "    butterfly_loss = torch.tensor(0.)\n",
    "    for elem in X_train_scaled:\n",
    "        y= model.forward(elem)\n",
    "        dydx = grad(y, elem, create_graph = True)[0]\n",
    "        dydT = dydx[T_loc]\n",
    "        if dydT < 0.0:\n",
    "            calendar_arbi_count += 1\n",
    "            calendar_loss += torch.exp(-dydT) * 1e-1\n",
    "    loss += calendar_loss\n",
    "    ## butterfly arbitrage\n",
    "    butterfly_count = 0\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "    if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_count += 1\n",
    "        butterfly_loss += torch.exp(-butter_ineq) * 1e-1    \n",
    "    loss += butterfly_loss\n",
    "\n",
    "    # regularizations\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "        loss += l2_lambda * l2_reg\n",
    "    loss_arr.append(loss)\n",
    "    if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "    if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    " \n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "if save_net: torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
