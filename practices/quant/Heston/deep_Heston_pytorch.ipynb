{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from Heston_NN import Net,weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net = True\n",
    "#save_net = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "arbitrage_weight = [1e-3,1e-3]; \n",
    "l2_weight = 1e-5\n",
    "arbitrage_weight = [0.0,0.0]; \n",
    "#l2_weight = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>k</th>\n",
       "      <th>kappa</th>\n",
       "      <th>rho</th>\n",
       "      <th>sigma</th>\n",
       "      <th>theta</th>\n",
       "      <th>v0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.622222</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124587</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.178556</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124588</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.178556</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124589</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.178556</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124590</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.178556</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124591</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.178556</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124592 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           T    k   kappa       rho     sigma  theta        v0\n",
       "0       0.25 -0.2   0.001 -0.800000  0.001000  0.001  0.010000\n",
       "1       0.25 -0.2   0.001 -0.622222  0.001000  0.001  0.010000\n",
       "2       0.25 -0.2   0.001 -0.444444  0.001000  0.001  0.010000\n",
       "3       0.25 -0.2   0.001 -0.266667  0.001000  0.001  0.010000\n",
       "4       0.25 -0.2   0.001 -0.088889  0.001000  0.001  0.010000\n",
       "...      ...  ...     ...       ...       ...    ...       ...\n",
       "124587  2.00  0.2  20.000  0.088889  0.178556  0.001  0.447214\n",
       "124588  2.00  0.2  20.000  0.266667  0.178556  0.001  0.447214\n",
       "124589  2.00  0.2  20.000  0.444444  0.178556  0.001  0.447214\n",
       "124590  2.00  0.2  20.000  0.622222  0.178556  0.001  0.447214\n",
       "124591  2.00  0.2  20.000  0.800000  0.178556  0.001  0.447214\n",
       "\n",
       "[124592 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_csv('Heston_data_input')\n",
    "X = table.drop(['C_price','delta'], axis=1)\n",
    "y = table[['C_price']]\n",
    "#X = df[['k','T','C_price']]\n",
    "#y = df[['v0','rho','kappa','theta','sigma']]\n",
    "T_loc = list(X.columns).index('T')\n",
    "k_loc = list(X.columns).index('k') # log strike \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_price</th>\n",
       "      <th>T</th>\n",
       "      <th>delta</th>\n",
       "      <th>k</th>\n",
       "      <th>kappa</th>\n",
       "      <th>rho</th>\n",
       "      <th>sigma</th>\n",
       "      <th>theta</th>\n",
       "      <th>v0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "      <td>124592.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.123665</td>\n",
       "      <td>0.798478</td>\n",
       "      <td>0.551715</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>5.546486</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.094902</td>\n",
       "      <td>3.193324</td>\n",
       "      <td>0.228621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.087584</td>\n",
       "      <td>0.557381</td>\n",
       "      <td>0.251790</td>\n",
       "      <td>0.127657</td>\n",
       "      <td>7.382655</td>\n",
       "      <td>0.515946</td>\n",
       "      <td>0.136277</td>\n",
       "      <td>3.561212</td>\n",
       "      <td>0.139527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.054625</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.048538</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.386104</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.107159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.116618</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.591939</td>\n",
       "      <td>-0.022222</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>0.089778</td>\n",
       "      <td>1.112000</td>\n",
       "      <td>0.252896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.184500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.731120</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>11.111556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.089778</td>\n",
       "      <td>6.667000</td>\n",
       "      <td>0.350055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.437569</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.072294</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             C_price              T          delta              k  \\\n",
       "count  124592.000000  124592.000000  124592.000000  124592.000000   \n",
       "mean        0.123665       0.798478       0.551715      -0.000008   \n",
       "std         0.087584       0.557381       0.251790       0.127657   \n",
       "min         0.001088       0.250000      -0.054625      -0.200000   \n",
       "25%         0.048538       0.250000       0.386104      -0.111111   \n",
       "50%         0.116618       0.750000       0.591939      -0.022222   \n",
       "75%         0.184500       1.000000       0.731120       0.111111   \n",
       "max         0.437569       2.000000       1.072294       0.200000   \n",
       "\n",
       "               kappa            rho          sigma          theta  \\\n",
       "count  124592.000000  124592.000000  124592.000000  124592.000000   \n",
       "mean        5.546486      -0.000011       0.094902       3.193324   \n",
       "std         7.382655       0.515946       0.136277       3.561212   \n",
       "min         0.001000      -0.800000       0.001000       0.001000   \n",
       "25%         0.001000      -0.444444       0.001000       0.001000   \n",
       "50%         0.001000      -0.088889       0.089778       1.112000   \n",
       "75%        11.111556       0.444444       0.089778       6.667000   \n",
       "max        20.000000       0.800000       0.800000      10.000000   \n",
       "\n",
       "                  v0  \n",
       "count  124592.000000  \n",
       "mean        0.228621  \n",
       "std         0.139527  \n",
       "min         0.010000  \n",
       "25%         0.107159  \n",
       "50%         0.252896  \n",
       "75%         0.350055  \n",
       "max         0.447214  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "...     X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "...     X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "input_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "X_val_scaled = input_scaler.transform(X_val)\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "\n",
    "#X_train_scaled = torch.FloatTensor(X_train_scaled,requires_grad=True)\n",
    "#X_test_scaled = torch.FloatTensor(X_test_scaled,requires_grad=True)\n",
    "X_train_scaled = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_train_scaled.requires_grad = True\n",
    "X_val_scaled = torch.FloatTensor(X_val_scaled).to(device)\n",
    "X_val_scaled.requires_grad = True\n",
    "X_test_scaled = torch.FloatTensor(X_test_scaled).to(device)\n",
    "X_test_scaled.requires_grad = True\n",
    "\n",
    "y_train = torch.FloatTensor(y_train.values).to(device)\n",
    "y_val = torch.FloatTensor(y_val.values).to(device)\n",
    "y_test = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "if save_net:\n",
    "    dump(input_scaler, open('input_scaler.pkl', 'wb'))\n",
    "#    input_scaler = load(open('input_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57142857, 2.5       , 0.0500025 , 0.625     , 1.25156446,\n",
       "       0.10001   , 2.28721158])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.42857143e-01,  5.00000000e-01, -5.00025001e-05,  5.00000000e-01,\n",
       "       -1.25156446e-03, -1.00010001e-04, -2.28721158e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_neurons=128,device=device)\n",
    "#model = Net(num_neurons=64,device=device)\n",
    "model.apply(weights_init)\n",
    "#model.load_state_dict(torch.load('heston_NN_intermediate')) # load model\n",
    "#model.load_state_dict(torch.load('heston_NN_final')) # load model\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  \n",
      "Loss: 0.02911433018743992\n",
      "Loss: 0.02899889647960663\n",
      "Epoch:  500  \n",
      "Loss: 0.026639901101589203\n",
      "Loss: 0.026532890275120735\n",
      "Epoch:  1000  \n",
      "Loss: 0.02435345947742462\n",
      "Loss: 0.024254698306322098\n",
      "Epoch:  1500  \n",
      "Loss: 0.022179778665304184\n",
      "Loss: 0.022090280428528786\n",
      "Epoch:  2000  \n",
      "Loss: 0.020170090720057487\n",
      "Loss: 0.02008969709277153\n",
      "Epoch:  2500  \n",
      "Loss: 0.018338419497013092\n",
      "Loss: 0.018267249688506126\n",
      "Epoch:  3000  \n",
      "Loss: 0.016666904091835022\n",
      "Loss: 0.016604576259851456\n",
      "Epoch:  3500  \n",
      "Loss: 0.015145525336265564\n",
      "Loss: 0.015092069283127785\n",
      "Epoch:  4000  \n",
      "Loss: 0.013770725578069687\n",
      "Loss: 0.013726010918617249\n",
      "Epoch:  4500  \n",
      "Loss: 0.012541232630610466\n",
      "Loss: 0.012505298480391502\n",
      "Epoch:  5000  \n",
      "Loss: 0.011453254148364067\n",
      "Loss: 0.011425749398767948\n",
      "Epoch:  5500  \n",
      "Loss: 0.010500486008822918\n",
      "Loss: 0.010481026023626328\n",
      "Epoch:  6000  \n",
      "Loss: 0.00967701431363821\n",
      "Loss: 0.009665429592132568\n",
      "Epoch:  6500  \n",
      "Loss: 0.00897327158600092\n",
      "Loss: 0.008969082497060299\n",
      "Epoch:  7000  \n",
      "Loss: 0.008375789038836956\n",
      "Loss: 0.008378518745303154\n",
      "Epoch:  7500  \n",
      "Loss: 0.007870052941143513\n",
      "Loss: 0.007879141718149185\n",
      "Epoch:  8000  \n",
      "Loss: 0.007442828733474016\n",
      "Loss: 0.00745747797191143\n",
      "Epoch:  8500  \n",
      "Loss: 0.007082831114530563\n",
      "Loss: 0.007102375850081444\n",
      "Epoch:  9000  \n",
      "Loss: 0.00678069144487381\n",
      "Loss: 0.006804235745221376\n",
      "Epoch:  9500  \n",
      "Loss: 0.006526446435600519\n",
      "Loss: 0.006553212180733681\n",
      "Epoch:  10000  \n",
      "Loss: 0.006308737210929394\n",
      "Loss: 0.006338128820061684\n",
      "Epoch:  10500  \n",
      "Loss: 0.0061173331923782825\n",
      "Loss: 0.006148694548755884\n",
      "Epoch:  11000  \n",
      "Loss: 0.0059442720375955105\n",
      "Loss: 0.0059771910309791565\n",
      "Epoch:  11500  \n",
      "Loss: 0.005784797947853804\n",
      "Loss: 0.005818856880068779\n",
      "Epoch:  12000  \n",
      "Loss: 0.0056365905329585075\n",
      "Loss: 0.005671459250152111\n",
      "Epoch:  12500  \n",
      "Loss: 0.005498850252479315\n",
      "Loss: 0.005534189287573099\n",
      "Epoch:  13000  \n",
      "Loss: 0.00537020293995738\n",
      "Loss: 0.005405817646533251\n",
      "Epoch:  13500  \n",
      "Loss: 0.005248752422630787\n",
      "Loss: 0.005284500773996115\n",
      "Epoch:  14000  \n",
      "Loss: 0.005133182741701603\n",
      "Loss: 0.005168964620679617\n",
      "Epoch:  14500  \n",
      "Loss: 0.005022631026804447\n",
      "Loss: 0.00505834212526679\n",
      "Epoch:  15000  \n",
      "Loss: 0.004916078876703978\n",
      "Loss: 0.004951614886522293\n",
      "Epoch:  15500  \n",
      "Loss: 0.00481279706582427\n",
      "Loss: 0.004848070442676544\n",
      "Epoch:  16000  \n",
      "Loss: 0.004712095949798822\n",
      "Loss: 0.004747036378830671\n",
      "Epoch:  16500  \n",
      "Loss: 0.004613480530679226\n",
      "Loss: 0.004648022353649139\n",
      "Epoch:  17000  \n",
      "Loss: 0.004516515880823135\n",
      "Loss: 0.004550591576844454\n",
      "Epoch:  17500  \n",
      "Loss: 0.0044208699837327\n",
      "Loss: 0.004454422276467085\n",
      "Epoch:  18000  \n",
      "Loss: 0.0043260506354272366\n",
      "Loss: 0.004359093960374594\n",
      "Epoch:  18500  \n",
      "Loss: 0.004232174251228571\n",
      "Loss: 0.004264703951776028\n",
      "Epoch:  19000  \n",
      "Loss: 0.004139766562730074\n",
      "Loss: 0.00417174119502306\n",
      "Epoch:  19500  \n",
      "Loss: 0.004049140494316816\n",
      "Loss: 0.004080539103597403\n",
      "Epoch:  20000  \n",
      "Loss: 0.003960375674068928\n",
      "Loss: 0.003991229459643364\n",
      "Epoch:  20500  \n",
      "Loss: 0.003873442532494664\n",
      "Loss: 0.003903765231370926\n",
      "Epoch:  21000  \n",
      "Loss: 0.0037885112687945366\n",
      "Loss: 0.0038182681892067194\n",
      "Epoch:  21500  \n",
      "Loss: 0.0037056514993309975\n",
      "Loss: 0.0037348249461501837\n",
      "Epoch:  22000  \n",
      "Loss: 0.003624885343015194\n",
      "Loss: 0.0036534909158945084\n",
      "Epoch:  22500  \n",
      "Loss: 0.0035463289823383093\n",
      "Loss: 0.0035743634216487408\n",
      "Epoch:  23000  \n",
      "Loss: 0.003469881834462285\n",
      "Loss: 0.003497373778373003\n",
      "Epoch:  23500  \n",
      "Loss: 0.003395259380340576\n",
      "Loss: 0.003422228619456291\n",
      "Epoch:  24000  \n",
      "Loss: 0.0033224045764654875\n",
      "Loss: 0.0033488336484879255\n",
      "Epoch:  24500  \n",
      "Loss: 0.003251485526561737\n",
      "Loss: 0.0032774272840470076\n",
      "Epoch:  25000  \n",
      "Loss: 0.003183347638696432\n",
      "Loss: 0.0032087755389511585\n",
      "Epoch:  25500  \n",
      "Loss: 0.003118191147223115\n",
      "Loss: 0.0031431776005774736\n",
      "Epoch:  26000  \n",
      "Loss: 0.0030558195430785418\n",
      "Loss: 0.0030803701374679804\n",
      "Epoch:  26500  \n",
      "Loss: 0.0029956011567264795\n",
      "Loss: 0.0030197296291589737\n",
      "Epoch:  27000  \n",
      "Loss: 0.0029373513534665108\n",
      "Loss: 0.0029610886704176664\n",
      "Epoch:  27500  \n",
      "Loss: 0.0028813849203288555\n",
      "Loss: 0.0029047683347016573\n",
      "Epoch:  28000  \n",
      "Loss: 0.0028284131549298763\n",
      "Loss: 0.0028514857403934\n",
      "Epoch:  28500  \n",
      "Loss: 0.002778315218165517\n",
      "Loss: 0.002801094437018037\n",
      "Epoch:  29000  \n",
      "Loss: 0.002730513922870159\n",
      "Loss: 0.0027530088555067778\n",
      "Epoch:  29500  \n",
      "Loss: 0.0026847003027796745\n",
      "Loss: 0.002706928411498666\n",
      "Epoch:  30000  \n",
      "Loss: 0.0026406431570649147\n",
      "Loss: 0.0026626160833984613\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ce6bdfeeff76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mloss_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mval_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-3) # 16\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3) # 16\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3) # 32\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3) \n",
    "epochs = 100001\n",
    "#epochs = 2001\n",
    "#epochs = 10\n",
    "loss_arr = []\n",
    "val_arr =[]\n",
    "freq = 500\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % freq == 0:\n",
    "        print('Epoch: ',i,' ')\n",
    "        show_log = True\n",
    "        if save_net: \n",
    "            torch.save(model.state_dict(), \"heston_NN_intermediate\") # save the model\n",
    "    else:\n",
    "        show_log = False\n",
    " \n",
    "#    loss = model.get_loss(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "#    val_loss = model.get_loss(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    loss = model.get_loss_test(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    val_loss = model.get_loss_test(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_arr.append(float(loss))\n",
    "    val_arr.append(float(val_loss))\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('loss')\n",
    "plt.plot(loss_arr,label='train')\n",
    "plt.plot(val_arr,label='validation')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_net:\n",
    "    X_scaled = input_scaler.transform(X)\n",
    "    X_scaled_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(table[['C_price']].values).to(device)\n",
    "    model.train()\n",
    "    for i in range(1000):\n",
    "        loss = model.get_loss_test(X_scaled_tensor, y_tensor, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    torch.save(model.state_dict(), \"heston_NN_final\") # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m2 = Net(num_neurons=64,device=device)\n",
    "#m2.load_state_dict(torch.load('heston_NN'))\n",
    "#m2.load_state_dict(torch.load('heston_NN_intermediate'))\n",
    "#m2.eval() # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.get_loss_test(X_test_scaled, y_test, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled[0].clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)\n",
    "dydx = grad(y,x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dydx[T_loc] < 0.0:\n",
    "    print('nono ',torch.exp(dydx[T_loc] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2 = grad(dydx[T_loc],x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled.clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_loss = torch.tensor(0.)\n",
    "butterfly_loss = torch.tensor(0.)\n",
    "for elem in X_train_scaled:\n",
    "    y= model.forward(elem)\n",
    "    dydx = grad(y, elem, create_graph = True)[0]\n",
    "    dydT = dydx[T_loc]\n",
    "    if dydT < 0.0:\n",
    "        calendar_arbi_count += 1\n",
    "        calendar_loss += torch.exp(-dydT[T_loc]) * 1e-1\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    if d2ydk2 - dydk < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_loss += torch.exp(-(d2ydk2 - dydk)) * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_input=7, num_neurons=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, num_neurons) \n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "#        x = F.relu(self.fc3(x))\n",
    "#        x = F.relu(self.fc4(x))\n",
    "#        x = F.relu(self.fc5(x))\n",
    "#        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # Penalization (loop over each data)\n",
    "        ## calendar arbitrage\n",
    "        calendar_arbi_count = 0\n",
    "        calendar_loss = torch.tensor(0.)\n",
    "        butterfly_loss = torch.tensor(0.)\n",
    "        for elem in x:\n",
    "            y= self.forward(elem)\n",
    "            dydx = grad(y, elem, create_graph = True)[0]\n",
    "            dydT = dydx[T_loc]\n",
    "            if dydT < 0.0:\n",
    "                calendar_arbi_count += 1\n",
    "                calendar_loss += torch.exp(-dydT) * arbitrage_weight[0]\n",
    "        loss += calendar_loss\n",
    "        ## butterfly arbitrage\n",
    "        butterfly_count = 0\n",
    "        dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "        d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "        # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "        # dCdK = 1/K*dcdk\n",
    "        # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "        butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "        if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "            butterfly_count += 1\n",
    "            butterfly_loss += torch.exp(-butter_ineq) * arbitrage_weight[1]\n",
    "        loss += butterfly_loss\n",
    "\n",
    "        # regularizations\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "        if show_log:\n",
    "            if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "            if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    "            print(f'Loss: {loss}')\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#        torch.nn.init.normal_(m.weight)\n",
    "#        xavier(m.weight.data)\n",
    "#        xavier(m.bias.data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "epochs = 30001\n",
    "#epochs = 101\n",
    "loss_arr = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "l2_lambda = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train_scaled)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    \n",
    "    # Penalization (loop over each data)\n",
    "    ## calendar arbitrage\n",
    "    calendar_arbi_count = 0\n",
    "    calendar_loss = torch.tensor(0.)\n",
    "    butterfly_loss = torch.tensor(0.)\n",
    "    for elem in X_train_scaled:\n",
    "        y= model.forward(elem)\n",
    "        dydx = grad(y, elem, create_graph = True)[0]\n",
    "        dydT = dydx[T_loc]\n",
    "        if dydT < 0.0:\n",
    "            calendar_arbi_count += 1\n",
    "            calendar_loss += torch.exp(-dydT) * 1e-1\n",
    "    loss += calendar_loss\n",
    "    ## butterfly arbitrage\n",
    "    butterfly_count = 0\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "    if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_count += 1\n",
    "        butterfly_loss += torch.exp(-butter_ineq) * 1e-1    \n",
    "    loss += butterfly_loss\n",
    "\n",
    "    # regularizations\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "        loss += l2_lambda * l2_reg\n",
    "    loss_arr.append(loss)\n",
    "    if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "    if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    " \n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "if save_net: torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
