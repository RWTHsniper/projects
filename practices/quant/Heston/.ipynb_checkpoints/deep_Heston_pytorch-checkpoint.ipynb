{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from Heston_NN import Net,weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net = True\n",
    "#save_net = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "arbitrage_weight = [1e-3,1e-3]; \n",
    "l2_weight = 1e-5\n",
    "arbitrage_weight = [0.0,0.0]; \n",
    "#l2_weight = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>T</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>kappa</th>\n",
       "      <th>theta</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.448511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>14.286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5686 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        k    T        v0       rho   kappa   theta   sigma\n",
       "0    -0.2  1.0  0.316228 -1.000000   0.001   0.001   0.001\n",
       "1    -0.2  1.0  0.316228 -0.714286   0.001   0.001   0.001\n",
       "2    -0.2  1.0  0.316228 -0.428571   0.001   0.001   0.001\n",
       "3    -0.2  1.0  0.316228 -0.142857   0.001   0.001   0.001\n",
       "4    -0.2  1.0  0.316228  0.142857   0.001   0.001   0.001\n",
       "...   ...  ...       ...       ...     ...     ...     ...\n",
       "5681  0.2  1.0  0.415440  1.000000  20.000  10.000  14.286\n",
       "5682  0.2  1.0  0.448511  1.000000  20.000  10.000  14.286\n",
       "5683  0.2  1.0  0.481581  1.000000  20.000  10.000  14.286\n",
       "5684  0.2  1.0  0.514652  1.000000  20.000  10.000  14.286\n",
       "5685  0.2  1.0  0.547723  1.000000  20.000  10.000  14.286\n",
       "\n",
       "[5686 rows x 7 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_csv('Heston_data_input')\n",
    "X = table.drop(['C_price','delta'], axis=1)\n",
    "y = table[['C_price']]\n",
    "#X = df[['k','T','C_price']]\n",
    "#y = df[['v0','rho','kappa','theta','sigma']]\n",
    "T_loc = list(X.columns).index('T')\n",
    "k_loc = list(X.columns).index('k') # log strike \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>T</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>kappa</th>\n",
       "      <th>theta</th>\n",
       "      <th>sigma</th>\n",
       "      <th>C_price</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.0</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "      <td>5686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.431894</td>\n",
       "      <td>0.268730</td>\n",
       "      <td>6.065719</td>\n",
       "      <td>4.605333</td>\n",
       "      <td>4.011151</td>\n",
       "      <td>0.240721</td>\n",
       "      <td>0.617825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.130813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075799</td>\n",
       "      <td>0.783798</td>\n",
       "      <td>7.585582</td>\n",
       "      <td>2.863153</td>\n",
       "      <td>5.019007</td>\n",
       "      <td>0.160363</td>\n",
       "      <td>0.412394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.085714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349298</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.429429</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.042445</td>\n",
       "      <td>0.073567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.028571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415440</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4.286286</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.834684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.429000</td>\n",
       "      <td>7.143143</td>\n",
       "      <td>8.572000</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.954826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.286000</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>1.140729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 k       T           v0          rho        kappa  \\\n",
       "count  5686.000000  5686.0  5686.000000  5686.000000  5686.000000   \n",
       "mean      0.006723     1.0     0.431894     0.268730     6.065719   \n",
       "std       0.130813     0.0     0.075799     0.783798     7.585582   \n",
       "min      -0.200000     1.0     0.316228    -1.000000     0.001000   \n",
       "25%      -0.085714     1.0     0.349298    -0.428571     0.001000   \n",
       "50%       0.028571     1.0     0.415440     0.428571     0.001000   \n",
       "75%       0.142857     1.0     0.481581     1.000000    11.429000   \n",
       "max       0.200000     1.0     0.547723     1.000000    20.000000   \n",
       "\n",
       "             theta        sigma      C_price        delta  \n",
       "count  5686.000000  5686.000000  5686.000000  5686.000000  \n",
       "mean      4.605333     4.011151     0.240721     0.617825  \n",
       "std       2.863153     5.019007     0.160363     0.412394  \n",
       "min       0.001000     0.001000     0.001001     0.001640  \n",
       "25%       1.429429     0.001000     0.042445     0.073567  \n",
       "50%       4.286286     0.001000     0.289540     0.834684  \n",
       "75%       7.143143     8.572000     0.372752     0.954826  \n",
       "max      10.000000    14.286000     0.499876     1.140729  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "...     X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "...     X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "input_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "X_val_scaled = input_scaler.transform(X_val)\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "\n",
    "#X_train_scaled = torch.FloatTensor(X_train_scaled,requires_grad=True)\n",
    "#X_test_scaled = torch.FloatTensor(X_test_scaled,requires_grad=True)\n",
    "X_train_scaled = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_train_scaled.requires_grad = True\n",
    "X_val_scaled = torch.FloatTensor(X_val_scaled).to(device)\n",
    "X_val_scaled.requires_grad = True\n",
    "X_test_scaled = torch.FloatTensor(X_test_scaled).to(device)\n",
    "X_test_scaled.requires_grad = True\n",
    "\n",
    "y_train = torch.FloatTensor(y_train.values).to(device)\n",
    "y_val = torch.FloatTensor(y_val.values).to(device)\n",
    "y_test = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "if save_net:\n",
    "    dump(input_scaler, open('input_scaler.pkl', 'wb'))\n",
    "#    input_scaler = load(open('input_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5       , 1.        , 4.31975162, 0.5       , 0.0500025 ,\n",
       "       0.10001   , 0.0700035 ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.00000000e-01, -1.00000000e+00, -1.36602540e+00,  5.00000000e-01,\n",
       "       -5.00025001e-05, -1.00010001e-04, -7.00035002e-05])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_neurons=128,device=device)\n",
    "model.apply(weights_init)\n",
    "#model.load_state_dict(torch.load('heston_NN_intermediate')) # load model\n",
    "#model.load_state_dict(torch.load('heston_NN_final')) # load model\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  \n",
      "Loss: 0.001393238897435367\n",
      "Loss: 0.0014337975298985839\n",
      "Epoch:  500  \n",
      "Loss: 0.0013932420406490564\n",
      "Loss: 0.0014338004402816296\n",
      "Epoch:  1000  \n",
      "Loss: 0.0013932441361248493\n",
      "Loss: 0.0014338023029267788\n",
      "Epoch:  1500  \n",
      "Loss: 0.0013932462316006422\n",
      "Loss: 0.0014338042819872499\n",
      "Epoch:  2000  \n",
      "Loss: 0.0013932482106611133\n",
      "Loss: 0.0014338059118017554\n",
      "Epoch:  2500  \n",
      "Loss: 0.001393249724060297\n",
      "Loss: 0.001433807541616261\n",
      "Epoch:  3000  \n",
      "Loss: 0.0013932514702901244\n",
      "Loss: 0.001433808822184801\n",
      "Epoch:  3500  \n",
      "Loss: 0.0013932528672739863\n",
      "Loss: 0.001433810219168663\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-f9ffc4fdbd5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#    loss = model.get_loss(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#    val_loss = model.get_loss(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marbitrage_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marbitrage_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\projects\\practices\\quant\\Heston\\Heston_NN.py\u001b[0m in \u001b[0;36mget_loss_test\u001b[1;34m(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0ml2_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcalendar_arbi_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3) # 16\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3) # 32\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3) \n",
    "epochs = 100001\n",
    "#epochs = 2001\n",
    "loss_arr = []\n",
    "val_arr =[]\n",
    "freq = 500\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % freq == 0:\n",
    "        print('Epoch: ',i,' ')\n",
    "        show_log = True\n",
    "        if save_net: \n",
    "            torch.save(model.state_dict(), \"heston_NN_intermediate\") # save the model\n",
    "    else:\n",
    "        show_log = False\n",
    " \n",
    "#    loss = model.get_loss(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "#    val_loss = model.get_loss(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    loss = model.get_loss_test(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    val_loss = model.get_loss_test(X_val_scaled, y_val, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_arr.append(float(loss))\n",
    "    val_arr.append(float(val_loss))\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('loss')\n",
    "plt.plot(loss_arr,label='train')\n",
    "plt.plot(val_arr,label='validation')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_net:     \n",
    "    X_scaled = input_scaler.transform(X)\n",
    "    X_scaled_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(table[['C_price']].values).to(device)\n",
    "    model.train()\n",
    "    for i in range(1000):\n",
    "        loss = model.get_loss_test(X_scaled_tensor, y_tensor, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    torch.save(model.state_dict(), \"heston_NN_final\") # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m2 = Net()\n",
    "#m2.load_state_dict(torch.load('heston_NN'))\n",
    "#2.eval() # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar  845   tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss: 0.0017984583973884583\n"
     ]
    }
   ],
   "source": [
    "loss = model.get_loss(X_test_scaled, y_test, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled[0].clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)\n",
    "dydx = grad(y,x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dydx[T_loc] < 0.0:\n",
    "    print('nono ',torch.exp(dydx[T_loc] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2 = grad(dydx[T_loc],x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6099e-06, -2.2349e-08, -7.7773e-07,  4.5950e-06, -4.4168e-06,\n",
       "         1.5403e-06,  1.6534e-06], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2ydx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled.clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calendar_arbi_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2802136ebb47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdydT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdydx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_loc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdydT\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mcalendar_arbi_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mcalendar_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdydT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_loc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdydk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdydx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_loc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# dCdk w.r.t. log-strike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calendar_arbi_count' is not defined"
     ]
    }
   ],
   "source": [
    "calendar_loss = torch.tensor(0.)\n",
    "butterfly_loss = torch.tensor(0.)\n",
    "for elem in X_train_scaled:\n",
    "    y= model.forward(elem)\n",
    "    dydx = grad(y, elem, create_graph = True)[0]\n",
    "    dydT = dydx[T_loc]\n",
    "    if dydT < 0.0:\n",
    "        calendar_arbi_count += 1\n",
    "        calendar_loss += torch.exp(-dydT[T_loc]) * 1e-1\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    if d2ydk2 - dydk < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_loss += torch.exp(-(d2ydk2 - dydk)) * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_input=7, num_neurons=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, num_neurons) \n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "#        x = F.relu(self.fc3(x))\n",
    "#        x = F.relu(self.fc4(x))\n",
    "#        x = F.relu(self.fc5(x))\n",
    "#        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # Penalization (loop over each data)\n",
    "        ## calendar arbitrage\n",
    "        calendar_arbi_count = 0\n",
    "        calendar_loss = torch.tensor(0.)\n",
    "        butterfly_loss = torch.tensor(0.)\n",
    "        for elem in x:\n",
    "            y= self.forward(elem)\n",
    "            dydx = grad(y, elem, create_graph = True)[0]\n",
    "            dydT = dydx[T_loc]\n",
    "            if dydT < 0.0:\n",
    "                calendar_arbi_count += 1\n",
    "                calendar_loss += torch.exp(-dydT) * arbitrage_weight[0]\n",
    "        loss += calendar_loss\n",
    "        ## butterfly arbitrage\n",
    "        butterfly_count = 0\n",
    "        dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "        d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "        # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "        # dCdK = 1/K*dcdk\n",
    "        # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "        butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "        if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "            butterfly_count += 1\n",
    "            butterfly_loss += torch.exp(-butter_ineq) * arbitrage_weight[1]\n",
    "        loss += butterfly_loss\n",
    "\n",
    "        # regularizations\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "        if show_log:\n",
    "            if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "            if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    "            print(f'Loss: {loss}')\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#        torch.nn.init.normal_(m.weight)\n",
    "#        xavier(m.weight.data)\n",
    "#        xavier(m.bias.data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "epochs = 30001\n",
    "#epochs = 101\n",
    "loss_arr = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "l2_lambda = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train_scaled)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    \n",
    "    # Penalization (loop over each data)\n",
    "    ## calendar arbitrage\n",
    "    calendar_arbi_count = 0\n",
    "    calendar_loss = torch.tensor(0.)\n",
    "    butterfly_loss = torch.tensor(0.)\n",
    "    for elem in X_train_scaled:\n",
    "        y= model.forward(elem)\n",
    "        dydx = grad(y, elem, create_graph = True)[0]\n",
    "        dydT = dydx[T_loc]\n",
    "        if dydT < 0.0:\n",
    "            calendar_arbi_count += 1\n",
    "            calendar_loss += torch.exp(-dydT) * 1e-1\n",
    "    loss += calendar_loss\n",
    "    ## butterfly arbitrage\n",
    "    butterfly_count = 0\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "    if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_count += 1\n",
    "        butterfly_loss += torch.exp(-butter_ineq) * 1e-1    \n",
    "    loss += butterfly_loss\n",
    "\n",
    "    # regularizations\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "        loss += l2_lambda * l2_reg\n",
    "    loss_arr.append(loss)\n",
    "    if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "    if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    " \n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "if save_net: torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
