{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('Heston_data_input')\n",
    "X = table.drop(['C_price','delta'], axis=1)\n",
    "y = table[['C_price']]\n",
    "#X = df[['k','T','C_price']]\n",
    "#y = df[['v0','rho','kappa','theta','sigma']]\n",
    "T_loc = list(X.columns).index('T')\n",
    "k_loc = list(X.columns).index('k') # log strike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63.000000\n",
       "mean      0.285844\n",
       "std       0.134396\n",
       "min       0.005952\n",
       "25%       0.251236\n",
       "50%       0.289855\n",
       "75%       0.381712\n",
       "max       0.452502\n",
       "Name: C_price, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['C_price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.2, random_state=42)\n",
    "input_scaler = preprocessing.MinMaxScaler()\n",
    "output_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "#y_train_scaled = output_scaler.fit_transform(y_train)\n",
    "#y_train_scaled = y_train # no scaling\n",
    "#X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0],X_train_scaled.shape[1],1) # for lstm\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "#y_test_scaled = y_test # no scaling\n",
    "\n",
    "#X_train_scaled = torch.FloatTensor(X_train_scaled,requires_grad=True)\n",
    "#X_test_scaled = torch.FloatTensor(X_test_scaled,requires_grad=True)\n",
    "X_train_scaled = torch.FloatTensor(X_train_scaled)\n",
    "X_train_scaled.requires_grad = True\n",
    "X_test_scaled = torch.FloatTensor(X_test_scaled)\n",
    "X_test_scaled.requires_grad = True\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "y_test = torch.FloatTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_neurons = 128\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_input=7):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, num_neurons) \n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "#        x = F.relu(self.fc3(x))\n",
    "#        x = F.relu(self.fc4(x))\n",
    "#        x = F.relu(self.fc5(x))\n",
    "#        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return x\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#        torch.nn.init.normal_(m.weight)\n",
    "#        xavier(m.weight.data)\n",
    "#        xavier(m.bias.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.apply(weights_init)\n",
    "        \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.10169577598571777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golde\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500 Loss: 0.003935719840228558\n",
      "Epoch: 1000 Loss: 0.002222724026069045\n",
      "Epoch: 1500 Loss: 0.0019023290369659662\n",
      "Epoch: 2000 Loss: 0.0017319407779723406\n",
      "Epoch: 2500 Loss: 0.001621306873857975\n",
      "Epoch: 3000 Loss: 0.001541770063340664\n",
      "Epoch: 3500 Loss: 0.001477908925153315\n",
      "Epoch: 4000 Loss: 0.0014213137328624725\n",
      "Epoch: 4500 Loss: 0.0013679189141839743\n",
      "Epoch: 5000 Loss: 0.0013155543711036444\n",
      "Epoch: 5500 Loss: 0.001263322657905519\n",
      "Epoch: 6000 Loss: 0.0012109517119824886\n",
      "Epoch: 6500 Loss: 0.0011582517763599753\n",
      "Epoch: 7000 Loss: 0.0011055199429392815\n",
      "Epoch: 7500 Loss: 0.0010528801940381527\n",
      "Epoch: 8000 Loss: 0.0010007033124566078\n",
      "Epoch: 8500 Loss: 0.0009490797528997064\n",
      "Epoch: 9000 Loss: 0.0008982847793959081\n",
      "Epoch: 9500 Loss: 0.0008485986618325114\n",
      "Epoch: 10000 Loss: 0.0008000796660780907\n",
      "Epoch: 10500 Loss: 0.0007537497440353036\n",
      "Epoch: 11000 Loss: 0.000707382510881871\n",
      "Epoch: 11500 Loss: 0.0006634005112573504\n",
      "Epoch: 12000 Loss: 0.0006214776658453047\n",
      "Epoch: 12500 Loss: 0.0005819733487442136\n",
      "calendar  12587   1   tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "Epoch: 13000 Loss: 0.000545360438991338\n",
      "Epoch: 13500 Loss: 0.0005108777550049126\n",
      "calendar  13858   1   tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "Epoch: 14000 Loss: 0.0004782496253028512\n",
      "Epoch: 14500 Loss: 0.0004476158937904984\n",
      "Epoch: 15000 Loss: 0.00041949766455218196\n",
      "Epoch: 15500 Loss: 0.0003937130095437169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-da81a72ff818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mbutterfly_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mdydx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mdydT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdydx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_loc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e7d36c232f83>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Max pooling over a (2, 2) window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30001\n",
    "#epochs = 101\n",
    "loss_arr = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "l2_lambda = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train_scaled)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    \n",
    "    # Penalization (loop over each data)\n",
    "    ## calendar arbitrage\n",
    "    calendar_arbi_count = 0\n",
    "    calendar_loss = torch.tensor(0.)\n",
    "    butterfly_loss = torch.tensor(0.)\n",
    "    for elem in X_train_scaled:\n",
    "        y= model.forward(elem)\n",
    "        dydx = grad(y, elem, create_graph = True)[0]\n",
    "        dydT = dydx[T_loc]\n",
    "        if dydT < 0.0:\n",
    "            calendar_arbi_count += 1\n",
    "            calendar_loss += torch.exp(-dydT) * 1e-1\n",
    "    loss += calendar_loss\n",
    "    ## butterfly arbitrage\n",
    "    butterfly_count = 0\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "    if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_count += 1\n",
    "        butterfly_loss += torch.exp(-butter_ineq) * 1e-1    \n",
    "    loss += butterfly_loss\n",
    "\n",
    "    # regularizations\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "        loss += l2_lambda * l2_reg\n",
    "    loss_arr.append(loss)\n",
    "    if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "    if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    " \n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_arr)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Difference Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = X_train_scaled[0].clone().detach()\n",
    "dx = 1e-3\n",
    "inp1[0] = torch.add(inp1[0],dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0002], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out1= model.forward(inp1)\n",
    "dy = out1 - out\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dydx = dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st diff FD  tensor([-0.1652], grad_fn=<DivBackward0>)\n",
      "2nd diff FD  tensor([0.0596], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2nd order diff\n",
    "x = []\n",
    "ind = 0\n",
    "tmp = X_train_scaled[0].clone().detach(); tmp.requires_grad = True\n",
    "tmp[ind] -= dx\n",
    "x.append(tmp)\n",
    "tmp = X_train_scaled[0].clone().detach(); tmp.requires_grad = True\n",
    "x.append(tmp)\n",
    "tmp = X_train_scaled[0].clone().detach(); tmp.requires_grad = True\n",
    "tmp[ind] += dx\n",
    "x.append(tmp)\n",
    "y = []\n",
    "for elem in x:\n",
    "    y.append(model.forward(elem))\n",
    "\n",
    "dydx_fd = (y[2] - y[1]) / dx\n",
    "d2ydx2_fd = (y[2] + y[0] - 2.*y[1]) / dx**2\n",
    "print('1st diff FD ',dydx_fd)\n",
    "print('2nd diff FD ',d2ydx2_fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled[0].clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)\n",
    "dydx = grad(y,x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dydx[T_loc] < 0.0:\n",
    "    print('nono ',torch.exp(dydx[T_loc] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2 = grad(dydx[T_loc],x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.7872e-05, -1.2255e-05, -3.9792e-05, -3.3263e-08,  1.1301e-04,\n",
       "         1.4363e-06,  1.0488e-04], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2ydx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled.clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_loss = torch.tensor(0.)\n",
    "butterfly_loss = torch.tensor(0.)\n",
    "for elem in X_train_scaled:\n",
    "    y= model.forward(elem)\n",
    "    dydx = grad(y, elem, create_graph = True)[0]\n",
    "    dydT = dydx[T_loc]\n",
    "    if dydT < 0.0:\n",
    "        calendar_arbi_count += 1\n",
    "        calendar_loss += torch.exp(-dydT[T_loc]) * 1e-1\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    if d2ydk2 - dydk < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_loss += torch.exp(-(d2ydk2 - dydk)) * 1e-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
