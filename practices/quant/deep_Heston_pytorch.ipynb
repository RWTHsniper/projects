{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "from Heston_NN import Net,weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net = True\n",
    "#save_net = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('Heston_data_input')\n",
    "X = table.drop(['C_price','delta'], axis=1)\n",
    "y = table[['C_price']]\n",
    "#X = df[['k','T','C_price']]\n",
    "#y = df[['v0','rho','kappa','theta','sigma']]\n",
    "T_loc = list(X.columns).index('T')\n",
    "k_loc = list(X.columns).index('k') # log strike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63.000000\n",
       "mean      0.285844\n",
       "std       0.134396\n",
       "min       0.005952\n",
       "25%       0.251236\n",
       "50%       0.289855\n",
       "75%       0.381712\n",
       "max       0.452502\n",
       "Name: C_price, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['C_price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.2, random_state=42)\n",
    "input_scaler = preprocessing.MinMaxScaler()\n",
    "output_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "#y_train_scaled = output_scaler.fit_transform(y_train)\n",
    "#y_train_scaled = y_train # no scaling\n",
    "#X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0],X_train_scaled.shape[1],1) # for lstm\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "#y_test_scaled = y_test # no scaling\n",
    "\n",
    "#X_train_scaled = torch.FloatTensor(X_train_scaled,requires_grad=True)\n",
    "#X_test_scaled = torch.FloatTensor(X_test_scaled,requires_grad=True)\n",
    "X_train_scaled = torch.FloatTensor(X_train_scaled)\n",
    "X_train_scaled.requires_grad = True\n",
    "X_test_scaled = torch.FloatTensor(X_test_scaled)\n",
    "X_test_scaled.requires_grad = True\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "y_test = torch.FloatTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.apply(weights_init)\n",
    "        \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  \n",
      "Loss: 0.0002062129497062415\n",
      "Epoch:  500  \n",
      "Loss: 0.00020476855570450425\n",
      "Epoch:  1000  \n",
      "Loss: 0.00020466979185584933\n",
      "Epoch:  1500  \n",
      "Loss: 0.00020499498350545764\n",
      "Epoch:  2000  \n",
      "Loss: 0.00020549414330162108\n",
      "Epoch:  2500  \n",
      "Loss: 0.00020464800763875246\n",
      "Epoch:  3000  \n",
      "Loss: 0.00020453146134968847\n",
      "Epoch:  3500  \n",
      "Loss: 0.0002046154986601323\n",
      "Epoch:  4000  \n",
      "Loss: 0.00020472946926020086\n",
      "Epoch:  4500  \n",
      "Loss: 0.00020495426724664867\n",
      "Epoch:  5000  \n",
      "Loss: 0.00020455093181226403\n",
      "Epoch:  5500  \n",
      "Loss: 0.0002044573047896847\n",
      "Epoch:  6000  \n",
      "Loss: 0.00020450055308174342\n",
      "Epoch:  6500  \n",
      "Loss: 0.0002047481102636084\n",
      "Epoch:  7000  \n",
      "Loss: 0.00020449479052331299\n",
      "Epoch:  7500  \n",
      "Loss: 0.00020444460096769035\n",
      "Epoch:  8000  \n",
      "Loss: 0.00020469969604164362\n",
      "Epoch:  8500  \n",
      "Loss: 0.00020491835311986506\n",
      "Epoch:  9000  \n",
      "Loss: 0.00020436539489310235\n",
      "Epoch:  9500  \n",
      "Loss: 0.00020449102157726884\n",
      "Epoch:  10000  \n",
      "Loss: 0.0002041491970885545\n"
     ]
    }
   ],
   "source": [
    "arbitrage_weight = [0.1,0.1]; l2_weight = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "epochs = 30001\n",
    "#epochs = 1001\n",
    "loss_arr = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % 500 == 0:\n",
    "        print('Epoch: ',i,' ')\n",
    "        show_log = True\n",
    "    else:\n",
    "        show_log = False\n",
    " \n",
    "    loss = model.get_loss(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    loss_arr.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAATcUlEQVR4nO3dbZBcVZ3H8d9/ZjIDJGEgMMSQZJhgMJpdFHREQMsnRIMScV11SWkhbnaju+vWPrywQumbfcWutbW1a0nJxhVddwVEZDVEXLR8KJGlAhNFDITIQAiZEDN5IJOEhMlM939f9O2h0+keerr7Tp/b5/upmpq+p++9ff63u3/dfe7t2+buAgDEoaPVHQAAzB5CHwAiQugDQEQIfQCICKEPABHpanUHpnPuuef6wMBAq7sBAJmyZcuW/e7eV+m6oEN/YGBAQ0NDre4GAGSKme2sdh3DOwAQEUIfACJC6ANARAh9AIgIoQ8AESH0ASAihD4ARCTo4/QBICT5vOuuoV3Ku/SxwSXq6sze++YgQ9/MVktavXz58lZ3BQCm3PPr3Vp/z28lSccnclr7tmUt7tHMBfky5e73uvu63t7eVncFAKYcOnai4uUsCTL0AQDpIPQBICKEPgBEhNAHgIgQ+gAQEUIfACJC6ANARAh9AIgIoQ8AESH0ASAihD4ARITQB4AamdnUZfcWdqQBhD4ARITQB4CIEPoAEJFZC30zu9DMvmZmd8/WbQIATlZT6JvZbWY2amZby9pXmdl2Mxs2s/XTrcPdn3H3tY10FgDQmFrf6X9D0qrSBjPrlHSLpGskrZS0xsxWmtnFZrap7O+8pva6RYZHj+q2X+5odTcAoG41/Uauu//CzAbKmi+TNOzuz0iSmd0p6Tp3v1nStfV2yMzWSVonSf39/fWuJhV/dMuDOjI+qRuvHFBHh73yAgAQmEbG9BdL2lUyPZK0VWRm55jZrZIuNbObqs3n7hvcfdDdB/v6+hroXvMdGZ+UJBl5DyCjanqnX0Wl6Kv6dQV3PyDpMw3cHgCgQY280x+RtLRkeomk5xvrToGZrTazDWNjY81YHQAg0UjoPyLpIjNbZmbdkq6XtLEZnXL3e919XW9vbzNWBwBI1HrI5h2SHpK0wsxGzGytu09K+qyk+yVtk3SXuz+eXlcBIBxZ3bdX69E7a6q03yfpvqb2CAAygBOuNRFj+gCQjiBDnzF9AEhHkKEPAEhHkKHP8A4ApCPI0A99eCerO3AAIMjQD1VWD9ECgCJCHwAiQugDQESCDH125AJAOoIM/dB35AJAVgUZ+gCAdBD6AFCjdjiAj9AHgDp49d+MClqQoc+OXABIR5Chz45cAEhHkKEfKk6/ACDrCP06kP0AsorQnwHOvQMg6wh9AIhIkKHP0TsAkI4gQ5+jdwAgHUGGPgAgHYQ+AESE0AeAOlhGz8RD6ANARAh9AKgDJ1wDAASP0AeAiAQZ+qF/Ocs58xqAjAoy9EP9clY299UDwMuCDH0AQDoIfQCICKEPABEh9AEgIoQ+AESE0AeAGrXDr+cR+gAQEUJ/BvhKFoCsI/QBICJBhn7op2EAgKyejSXI0A/1NAxFGb2vASDM0A9VG+y4BxA5Qh8AIkLoA0BECH0AiAihDwARIfQBICKEPgBEhNAHgDpk9eRrhD4ARITQB4CIEPoAEBFCvw5ZPdESgObJag4Q+jNgWd1zAwAJQh8AIkLoA0BEZi30zexDZvZVM/u+mb13tm4XAPCymkLfzG4zs1Ez21rWvsrMtpvZsJmtn24d7v49d/9zSTdK+pO6ewwAqFtXjfN9Q9KXJX2z2GBmnZJukXS1pBFJj5jZRkmdkm4uW/5P3X00ufyFZDkAwCyrKfTd/RdmNlDWfJmkYXd/RpLM7E5J17n7zZKuLV+HFQ59+UdJP3T3X1W7LTNbJ2mdJPX399fSvVnjWT1GCwASjYzpL5a0q2R6JGmr5q8lvUfSR8zsM9VmcvcN7j7o7oN9fX0NdA8AmqsdDtqudXinkkr1V30r7O5fkvSlBm4PANCgRt7pj0haWjK9RNLzjXWnwMxWm9mGsbGxZqwOAJBoJPQfkXSRmS0zs25J10va2IxOufu97r6ut7e3GasDACRqPWTzDkkPSVphZiNmttbdJyV9VtL9krZJusvdH0+vq+Hw6qNYABC0Wo/eWVOl/T5J9zW1RwEzs+yeZQlAU2U1CYI8DQNj+gCQjiBDnzF9AEhHkKEPAEhHkKHP8A4ApCPI0Gd4BwDSEWToAwDSQegDQEQIfQCoQ1ZPvhZk6LMjFwDSEWTosyMXANIRZOiHjjMxAMgqQn8GsjqGBwBFhD4A1CGrH/iDDH125AJAOoIMfXbkAkA6ggz9UGX14xwAFBH6ABARQh8AIkLoA0BECH0AqJFZ9r+tE2Toc8gmAKQjyNDnkE0ASEeQoR+q7H+wAxA7Qh8AIkLoA0BECH0AqENWT7FO6ANARAh9AIhIkKHPcfoAkI4gQ5/j9AEgHUGGPgAgHYQ+ANQhq6fhIfQBICKEfh2yenwuABD6M5DVj3MAUEToA0BECH0AiAihDwARIfRngB24AIqymgdBhj6nYQCAdAQZ+pyGAQDSEWToAwDSQegDQEQIfQCICKEPADVqh2/lE/p1cGX0WC0A0SP0Z6AdXuUBxI3QB4CIEPoAEBFCHwAiQugDQEQIfQCICKEPAHXI6qHbhD4ARITQB4CIEPoAEJFZC30ze52Z3Wpmd5vZX8zW7QJAGkzZ/Ip+TaFvZreZ2aiZbS1rX2Vm281s2MzWT7cOd9/m7p+R9DFJg/V3ufWy+jNpAFDrO/1vSFpV2mBmnZJukXSNpJWS1pjZSjO72Mw2lf2dlyzzQUm/lPSTplUwi7L6yg4ARV21zOTuvzCzgbLmyyQNu/szkmRmd0q6zt1vlnRtlfVslLTRzH4g6fZK85jZOknrJKm/v7+W7gEAalRT6FexWNKukukRSW+pNrOZvVPShyX1SLqv2nzuvkHSBkkaHBwMaiAlq8flAkBRI6Ffaayjaiq6+88l/byB2wMANKiRo3dGJC0tmV4i6fnGulNgZqvNbMPY2FgzVgcASDQS+o9IusjMlplZt6TrJW1sRqfc/V53X9fb29uM1QEAErUesnmHpIckrTCzETNb6+6Tkj4r6X5J2yTd5e6Pp9dVAECjaj16Z02V9vs0zU5ZAGhXWT2wI8jTMDCmDwDpCDL0GdMHgHQEGfoAEKJ2+E4+oV+HbI7kAUCgoR/qmD7n3gGQdUGGPmP6AJCOIEMfAJAOQh8AIhJk6Ic6pg8AWRdk6DOmDwDpCDL0AQDpIPQBICKEPgDUI6Pf0gwy9NmRCwDpCDL02ZELAOkIMvRD557Rz3UAmiejZ2Uh9AEgIoQ+AESE0AeAiBD6ABCRIEOfQzYBIB1Bhj6HbAJAOoIMfQBAOgh9AIgIoQ8AESH0AaAeGf1iPqFfh4ze1wBA6M9IRs+1AaBJLPshEGToc5w+AKQjyNDnOH0ASEeQoQ8ASAehDwARIfQBICKEPgBEhNAHgIgQ+gAQkbYM/W17DuuRZw+2uhsAEJy2DP1r/u0BffTWh7Rl5wsaPfySxo5N6Oj4pF6ayGkil1cu73JP52QK7q5c3qcuu7smcvmTpouX80k/ipclaTKZt9I8pctLmrquuFxxHbX0rbQ/5esuXW9pH0rly2osX0fpdHH50tqK80zk8joxeXLN45O5U9ZRnC6dt9rtlNdRun2r1Vq+bcvvh9KaS+/f4nT59iq9vvR2y/swUWGblC6fq3D/V9v2le67orHjExX7MnZs4qTHTfmy5bWUb6fi/VHel5cmclWfY9W2RaX7oah8fdXmS9NLEzkdPzF5Ulvp8658m0mF7VvK3XV0/OR15PKu4ydy087TTDbbG24mBgcHfWhoaMbLDaz/Qc3zdpjUYSYzKe+FMy2YSSabOu2CqXC+ndLA6e7qkFzy5Ew8JpPLNZGb/e3Z09Wh8aRv3V0d6jRTZ4epw6TOjuJl0+iR8YZup1jzibKgqoWZVHyoze/p0pEKD+ruro5TQn229Z4+56SAnC2nz+nUZD6f+uOnu7Oj4v1X+hhq6u2VPR47OkyHjtW2fc/o7lSnWcXHSun6S5+zZsXnsBWevqXTZddZMsPL7aeuQ8V2k3YdPD6juksfyz1dhe0+k7gd+sJ7dO68ntoXKGFmW9x9sNJ1XXWtMWVmtlrS6uXLl9e1/Bf/+PX63Hcf07q3X6j+BWdofDKvXD6vybwrl3PlXcp7Ia7zeZ+63JEEk6v4P7mHXJJJo4fH9fCOg7r2DYskFV4sitwLD4zxibxue3CHPnTJ+freo89r+XnzNDx6VB++dLF+9MReHR2f1Ko/eJX+9/Hfn9Lv1y/p1WMjhVNPfPodF+r+rb/XsweOadm5c7Vj/4snzfuxwSW6a2hEknTDFRfoqw/skCRd/+al6unqUC5fqDGXd+WSd4e7Dx3XA0/t19q3LdPxiZxu3/ycFszt1sEXT0yt9wMXL9LQzoPae3hcH3j9Iv3gsT1T191w+QWa09WhXQePadNje/TaV83XmafP0cM7Th5K65vfo33JC0xnhymXd/3VO5fr6w/u0PzT5uj9Fy/S9x/drcm8TwXsjVcOqKvDtH3vET3w1H59+u0XKu8+Vde7VvTpgaf2azLvmtNpuuq1C6e24dzuTr1Y8k5Jki4bWKCHkyG+hWf2aO/hU1/w5vV0nfKO6kOXnK+j4zl991cjOmdut973h6/S7Zufm3oxePPA2Xr2wLGp+t61ok8/275Pr+6bq6tet1DfGdqlF8pCbfUbzteeQ8c1tPMFXfnqc/R/Tx/Qgrndet2i+Xpw+IBWLjpTl/SfpcPHJ7Qp2d5Xr1yoX+18QQdePKHze0/T82MvTW3Lal6zcJ5+t/eoJOndrz1PP31yVJL00Tct0Xe2jGheT5c+fnm/Nv1mj3YfOq6Pv6Vf39r83NT237zjoB7ddWhqfW9Y0qvfjIzpNQvn6anRoycF1kXnzdPCM0/TL4f36+qVC/XjJ/ZKKrx4zT+tS6NHxnX6nE598soB5fL5qcdj3l3ffGhn4T5atkC7Xziu3YdODdM1l/Vrbnencu7asf9F/Xz7Pp19xhxdd8liPbHnsB7ecVBdHaZPvXUgefNVfPd/8vO32GdPnuOl7cVpFacrXOfJyl3SroO7T+rjp946oK8/+KwkTd2vRe9a0afh0aN6el/heXvDFRfowNET+p9Hd8tdumTpWbrgnDM0p7NDd28pPI/f8Zo+9XR16EfJtkzrDVBbvtMHgJhN906/Lcf0AQCVEfoAEBFCHwAiQugDQEQIfQCICKEPABEh9AEgIoQ+AEQk6C9nmdk+STvrXPxcSfub2J0soOY4UHP7a7TeC9y9r9IVQYd+I8xsqNo30toVNceBmttfmvUyvAMAESH0ASAi7Rz6G1rdgRag5jhQc/tLrd62HdMHAJyqnd/pAwDKEPoAEJG2C30zW2Vm281s2MzWt7o/jTCzpWb2MzPbZmaPm9nfJO0LzOzHZvZU8v/skmVuSmrfbmbvK2l/k5n9NrnuS2YlP/sVGDPrNLNfm9mmZLqt65UkMzvLzO42syeT+/uKdq7bzP4ueUxvNbM7zOy0dqzXzG4zs1Ez21rS1rQ6zazHzL6dtG82s4FX7FS1HybO4p+kTklPS7pQUrek30ha2ep+NVDPIklvTC7Pl/Q7SSslfVHS+qR9vaR/Si6vTGrukbQs2RadyXUPS7pChZ///KGka1pd3zR1/72k2yVtSqbbut6kv/8p6c+Sy92SzmrXuiUtlrRD0unJ9F2SbmzHeiW9XdIbJW0taWtanZL+UtKtyeXrJX37FfvU6o3S5A18haT7S6ZvknRTq/vVxPq+L+lqSdslLUraFknaXqleSfcn22SRpCdL2tdI+vdW11OlxiWSfiLp3Xo59Nu23qR/ZyYhaGXtbVl3Evq7JC1Q4Xe6N0l6bxvXO1AW+k2rszhPcrlLhW/x2nT9abfhneKDqWgkacu85GPbpZI2S1ro7nskKfl/XjJbtfoXJ5fL20P0r5I+J6n0V6HbuV6p8Ml0n6SvJ8Na/2Fmc9Wmdbv7bkn/LOk5SXskjbn7j9Sm9VbQzDqnlnH3SUljks6Z7sbbLfQrjedl/phUM5sn6buS/tbdD083a4U2n6Y9KGZ2raRRd99S6yIV2jJTb4kuFYYAvuLul0p6UYWP/dVkuu5kDPs6FYYwzpc018w+Md0iFdoyU+8M1FPnjLdBu4X+iKSlJdNLJD3for40hZnNUSHwv+Xu9yTNe81sUXL9IkmjSXu1+keSy+XtoXmrpA+a2bOS7pT0bjP7b7VvvUUjkkbcfXMyfbcKLwLtWvd7JO1w933uPiHpHklXqn3rLdfMOqeWMbMuSb2SDk534+0W+o9IusjMlplZtwo7Nja2uE91S/bQf03SNnf/l5KrNkr6ZHL5kyqM9Rfbr0/26C+TdJGkh5OPkEfM7PJknTeULBMMd7/J3Ze4+4AK991P3f0TatN6i9z995J2mdmKpOkqSU+ofet+TtLlZnZG0s+rJG1T+9Zbrpl1lq7rIyo8Z6b/tNPqnRwp7DR5vwpHuTwt6fOt7k+DtbxNhY9qj0l6NPl7vwpjdj+R9FTyf0HJMp9Pat+ukiMZJA1K2ppc92W9ws6eVv9Jeqde3pEbQ72XSBpK7uvvSTq7neuW9A+Snkz6+l8qHLHSdvVKukOF/RYTKrwrX9vMOiWdJuk7koZVOMLnwlfqE6dhAICItNvwDgBgGoQ+AESE0AeAiBD6ABARQh8AIkLoA0BECH0AiMj/A77+DfuWORC0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if save_net: \n",
    "    torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "plt.plot(loss_arr)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = Net()\n",
    "m2.load_state_dict(torch.load('heston_NN'))\n",
    "m2.eval() # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00021026314061600715\n"
     ]
    }
   ],
   "source": [
    "loss = model.get_loss(X_test_scaled, y_test, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled[0].clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)\n",
    "dydx = grad(y,x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dydx[T_loc] < 0.0:\n",
    "    print('nono ',torch.exp(dydx[T_loc] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2 = grad(dydx[T_loc],x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled.clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_loss = torch.tensor(0.)\n",
    "butterfly_loss = torch.tensor(0.)\n",
    "for elem in X_train_scaled:\n",
    "    y= model.forward(elem)\n",
    "    dydx = grad(y, elem, create_graph = True)[0]\n",
    "    dydT = dydx[T_loc]\n",
    "    if dydT < 0.0:\n",
    "        calendar_arbi_count += 1\n",
    "        calendar_loss += torch.exp(-dydT[T_loc]) * 1e-1\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    if d2ydk2 - dydk < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_loss += torch.exp(-(d2ydk2 - dydk)) * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_input=7, num_neurons=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, num_neurons) \n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "#        x = F.relu(self.fc3(x))\n",
    "#        x = F.relu(self.fc4(x))\n",
    "#        x = F.relu(self.fc5(x))\n",
    "#        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # Penalization (loop over each data)\n",
    "        ## calendar arbitrage\n",
    "        calendar_arbi_count = 0\n",
    "        calendar_loss = torch.tensor(0.)\n",
    "        butterfly_loss = torch.tensor(0.)\n",
    "        for elem in x:\n",
    "            y= self.forward(elem)\n",
    "            dydx = grad(y, elem, create_graph = True)[0]\n",
    "            dydT = dydx[T_loc]\n",
    "            if dydT < 0.0:\n",
    "                calendar_arbi_count += 1\n",
    "                calendar_loss += torch.exp(-dydT) * arbitrage_weight[0]\n",
    "        loss += calendar_loss\n",
    "        ## butterfly arbitrage\n",
    "        butterfly_count = 0\n",
    "        dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "        d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "        # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "        # dCdK = 1/K*dcdk\n",
    "        # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "        butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "        if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "            butterfly_count += 1\n",
    "            butterfly_loss += torch.exp(-butter_ineq) * arbitrage_weight[1]\n",
    "        loss += butterfly_loss\n",
    "\n",
    "        # regularizations\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "        if show_log:\n",
    "            if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "            if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    "            print(f'Loss: {loss}')\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#        torch.nn.init.normal_(m.weight)\n",
    "#        xavier(m.weight.data)\n",
    "#        xavier(m.bias.data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "epochs = 30001\n",
    "#epochs = 101\n",
    "loss_arr = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "l2_lambda = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train_scaled)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    \n",
    "    # Penalization (loop over each data)\n",
    "    ## calendar arbitrage\n",
    "    calendar_arbi_count = 0\n",
    "    calendar_loss = torch.tensor(0.)\n",
    "    butterfly_loss = torch.tensor(0.)\n",
    "    for elem in X_train_scaled:\n",
    "        y= model.forward(elem)\n",
    "        dydx = grad(y, elem, create_graph = True)[0]\n",
    "        dydT = dydx[T_loc]\n",
    "        if dydT < 0.0:\n",
    "            calendar_arbi_count += 1\n",
    "            calendar_loss += torch.exp(-dydT) * 1e-1\n",
    "    loss += calendar_loss\n",
    "    ## butterfly arbitrage\n",
    "    butterfly_count = 0\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "    if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_count += 1\n",
    "        butterfly_loss += torch.exp(-butter_ineq) * 1e-1    \n",
    "    loss += butterfly_loss\n",
    "\n",
    "    # regularizations\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "        loss += l2_lambda * l2_reg\n",
    "    loss_arr.append(loss)\n",
    "    if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "    if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    " \n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "if save_net: torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
