{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "from Heston_NN import Net,weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net = True\n",
    "#save_net = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('Heston_data_input')\n",
    "X = table.drop(['C_price','delta'], axis=1)\n",
    "y = table[['C_price']]\n",
    "#X = df[['k','T','C_price']]\n",
    "#y = df[['v0','rho','kappa','theta','sigma']]\n",
    "T_loc = list(X.columns).index('T')\n",
    "k_loc = list(X.columns).index('k') # log strike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63.000000\n",
       "mean      0.285844\n",
       "std       0.134396\n",
       "min       0.005952\n",
       "25%       0.251236\n",
       "50%       0.289855\n",
       "75%       0.381712\n",
       "max       0.452502\n",
       "Name: C_price, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['C_price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.2, random_state=42)\n",
    "input_scaler = preprocessing.MinMaxScaler()\n",
    "output_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "#y_train_scaled = output_scaler.fit_transform(y_train)\n",
    "#y_train_scaled = y_train # no scaling\n",
    "#X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0],X_train_scaled.shape[1],1) # for lstm\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "#y_test_scaled = y_test # no scaling\n",
    "\n",
    "#X_train_scaled = torch.FloatTensor(X_train_scaled,requires_grad=True)\n",
    "#X_test_scaled = torch.FloatTensor(X_test_scaled,requires_grad=True)\n",
    "X_train_scaled = torch.FloatTensor(X_train_scaled)\n",
    "X_train_scaled.requires_grad = True\n",
    "X_test_scaled = torch.FloatTensor(X_test_scaled)\n",
    "X_test_scaled.requires_grad = True\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "y_test = torch.FloatTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc7): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.apply(weights_init)\n",
    "        \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  \n",
      "calendar  34   tensor(3.6248, grad_fn=<AddBackward0>)\n",
      "butterfly  1   tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Loss: 3.7599334716796875\n",
      "Epoch:  500  \n",
      "Loss: 0.0015573782147839665\n",
      "Epoch:  1000  \n",
      "Loss: 0.0013559574726969004\n",
      "Epoch:  1500  \n",
      "Loss: 0.001200366416014731\n",
      "Epoch:  2000  \n",
      "Loss: 0.001066751778125763\n",
      "Epoch:  2500  \n",
      "Loss: 0.0009480195585638285\n",
      "Epoch:  3000  \n",
      "Loss: 0.0008380989311262965\n",
      "Epoch:  3500  \n",
      "Loss: 0.0007372391410171986\n",
      "Epoch:  4000  \n",
      "Loss: 0.0006443352904170752\n",
      "Epoch:  4500  \n",
      "Loss: 0.0005568615160882473\n",
      "Epoch:  5000  \n",
      "Loss: 0.00047937192721292377\n",
      "Epoch:  5500  \n",
      "Loss: 0.00041167830931954086\n",
      "Epoch:  6000  \n",
      "Loss: 0.00035545791615732014\n",
      "Epoch:  6500  \n",
      "Loss: 0.0003088088415097445\n",
      "Epoch:  7000  \n",
      "Loss: 0.00027191900881007314\n",
      "Epoch:  7500  \n",
      "Loss: 0.00024471725919283926\n",
      "Epoch:  8000  \n",
      "Loss: 0.00022658854140900075\n",
      "Epoch:  8500  \n",
      "Loss: 0.0002158898423658684\n"
     ]
    }
   ],
   "source": [
    "arbitrage_weight = [0.1,0.1]; l2_weight = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "epochs = 10001\n",
    "#epochs = 1001\n",
    "loss_arr = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % 500 == 0:\n",
    "        print('Epoch: ',i,' ')\n",
    "        show_log = True\n",
    "    else:\n",
    "        show_log = False\n",
    " \n",
    "    loss = model.get_loss(X_train_scaled, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=show_log)\n",
    "    loss_arr.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_net: torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "plt.plot(loss_arr)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Net()\n",
    "m2.load_state_dict(torch.load('heston_NN'))\n",
    "m2.eval() # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.get_loss(X_test_scaled, y_test, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled[0].clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)\n",
    "dydx = grad(y,x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dydx[T_loc] < 0.0:\n",
    "    print('nono ',torch.exp(dydx[T_loc] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2 = grad(dydx[T_loc],x, create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2ydx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_scaled.clone().detach()\n",
    "x.requires_grad = True\n",
    "y= model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_loss = torch.tensor(0.)\n",
    "butterfly_loss = torch.tensor(0.)\n",
    "for elem in X_train_scaled:\n",
    "    y= model.forward(elem)\n",
    "    dydx = grad(y, elem, create_graph = True)[0]\n",
    "    dydT = dydx[T_loc]\n",
    "    if dydT < 0.0:\n",
    "        calendar_arbi_count += 1\n",
    "        calendar_loss += torch.exp(-dydT[T_loc]) * 1e-1\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    if d2ydk2 - dydk < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_loss += torch.exp(-(d2ydk2 - dydk)) * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,num_input=7, num_neurons=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, num_neurons) \n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "#        self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "#        x = F.relu(self.fc3(x))\n",
    "#        x = F.relu(self.fc4(x))\n",
    "#        x = F.relu(self.fc5(x))\n",
    "#        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, x, y_train, T_loc, k_loc, criterion, arbitrage_weight, l2_weight, show_log=False):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # Penalization (loop over each data)\n",
    "        ## calendar arbitrage\n",
    "        calendar_arbi_count = 0\n",
    "        calendar_loss = torch.tensor(0.)\n",
    "        butterfly_loss = torch.tensor(0.)\n",
    "        for elem in x:\n",
    "            y= self.forward(elem)\n",
    "            dydx = grad(y, elem, create_graph = True)[0]\n",
    "            dydT = dydx[T_loc]\n",
    "            if dydT < 0.0:\n",
    "                calendar_arbi_count += 1\n",
    "                calendar_loss += torch.exp(-dydT) * arbitrage_weight[0]\n",
    "        loss += calendar_loss\n",
    "        ## butterfly arbitrage\n",
    "        butterfly_count = 0\n",
    "        dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "        d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "        # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "        # dCdK = 1/K*dcdk\n",
    "        # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "        butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "        if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "            butterfly_count += 1\n",
    "            butterfly_loss += torch.exp(-butter_ineq) * arbitrage_weight[1]\n",
    "        loss += butterfly_loss\n",
    "\n",
    "        # regularizations\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "        if show_log:\n",
    "            if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "            if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    "            print(f'Loss: {loss}')\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "#        torch.nn.init.normal_(m.weight)\n",
    "#        xavier(m.weight.data)\n",
    "#        xavier(m.bias.data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "epochs = 30001\n",
    "#epochs = 101\n",
    "loss_arr = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "l2_lambda = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train_scaled)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    \n",
    "    # Penalization (loop over each data)\n",
    "    ## calendar arbitrage\n",
    "    calendar_arbi_count = 0\n",
    "    calendar_loss = torch.tensor(0.)\n",
    "    butterfly_loss = torch.tensor(0.)\n",
    "    for elem in X_train_scaled:\n",
    "        y= model.forward(elem)\n",
    "        dydx = grad(y, elem, create_graph = True)[0]\n",
    "        dydT = dydx[T_loc]\n",
    "        if dydT < 0.0:\n",
    "            calendar_arbi_count += 1\n",
    "            calendar_loss += torch.exp(-dydT) * 1e-1\n",
    "    loss += calendar_loss\n",
    "    ## butterfly arbitrage\n",
    "    butterfly_count = 0\n",
    "    dydk = dydx[k_loc] # dCdk w.r.t. log-strike \n",
    "    d2ydk2 = grad(dydx[k_loc],elem, create_graph = True)[0][k_loc] # d2Cdk2 w.r.t. log-strike\n",
    "    # butterfly arbitrage: d2CdK2 = 1/K^2(d2Cdk2 - dCdk) > 0, d2Cdk2 > dCdk\n",
    "    # dCdK = 1/K*dcdk\n",
    "    # d2CdK2 = 1/e^2k * (d2Cdk2 - dCdk)\n",
    "    butter_ineq = (d2ydk2 - dydk)/torch.exp(2.0*elem[k_loc])\n",
    "    if butter_ineq < 0.0: # violation of butterfly arbitrage\n",
    "        butterfly_count += 1\n",
    "        butterfly_loss += torch.exp(-butter_ineq) * 1e-1    \n",
    "    loss += butterfly_loss\n",
    "\n",
    "    # regularizations\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "        loss += l2_lambda * l2_reg\n",
    "    loss_arr.append(loss)\n",
    "    if calendar_arbi_count > 0: print('calendar ',i, ' ',calendar_arbi_count,' ',calendar_loss)\n",
    "    if butterfly_count > 0: print('butterfly ',i, ' ',butterfly_count,' ',butterfly_loss)\n",
    " \n",
    "    if i % 500 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "if save_net: torch.save(model.state_dict(), \"heston_NN\") # save the model\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
